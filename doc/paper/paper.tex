\documentclass[a4paper]{article}
%\def\year{2020}\relax
%\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
%\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
%\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage[margin=1in]{geometry}
\usepackage{csquotes}


% our packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{booktabs}
%\usepackage[inline]{enumitem}
%\usepackage{mathtools}
%\usepackage{multirow}
%\usepackage[mode=buildnew,subpreambles=true]{standalone}
\usepackage{subcaption}
%\usepackage{todonotes}
\usepackage[table]{xcolor}  % TODO Comment out for final submission
%\usepackage{pgfplots}
%\usepackage{tikz}
\usepackage{natbib}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{hyperref}


% Some useful macros
%\newcommand{\inlinecite}[1]{\citeauthor{#1}~(\citeyear{#1})}
%\newcommand{\citealp}[1]{\citeauthor{#1}~\citeyear{#1}}

\newcommand{\smallpar}[1]{{\vspace{10pt}\noindent \bf #1.}}

\newcommand{\free}[1]{\ensuremath{\mathrm{free}(#1)}}
\newcommand{\vars}{\ensuremath{\mathrm{vars}}}
\newcommand{\pre}{\ensuremath{\mathrm{pre}}}
\newcommand{\add}{\ensuremath{\mathrm{add}}}
\newcommand{\del}{\ensuremath{\mathrm{del}}}
\newcommand{\effs}{\ensuremath{\mathrm{effs}}}

\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\tuple}[1]{\tup{#1}}  % Just an alias
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\newcontent}{\textbf{\color{red}NEW}}

\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\newcommand{\wip}[1]{{\color{red} #1}}  % From "work in progress" :-)
\newcommand{\gfm}[1]{\footnote{\color{red}{[Guillem] #1}}}

\newcommand{\numtasks}[1]{\small{(#1)}}

\newcommand{\badtx}{\ensuremath{\mathrm{BAD}}}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title ()
/Author ()
} %Leave this

\title{Self-Supervised Learning of Generalized Policies in multi-agent domains}
%Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\author{GFM}

\begin{document}

\maketitle


\abstract

Generalized planning is concerned with the computation of general policies that solve multiple instances of a planning domain all at once. Recently, it has been recently shown how these policies can be computed for single-agent settings. These policies can be computed in two steps: first, a suitable abstraction in the form of a qualitative numerical planning problem (QNP) is learned from sample plans, then the general policies are obtained from the learned QNP using a planner. We introduce an alternative approach for computing more expressive general policies in multi-agent settings which does not require sample plans nor a QNP planner. The new formulation is very simple and can be cast in terms that are more standard in machine learning: a large but finite pool of features (functions) is defined from the predicates in the planning examples using a general grammar, and a small subset of features is sought for separating “good” from “bad” state transitions, as well as goals from non-goals. The problem of finding such a ``separating surface'' while labeling the transitions as ``good'' or ``bad'' are jointly addressed as a single combinatorial optimization problem expressed as a Weighted Max-SAT problem. As many domains have no general policies that are optimal for all instance, we argue that looking for a simple policy, which might have some performance reduction  is beneficial . We assume that the state space is discrete and also that we know what actions the agents can take in their turn. The approach yields general policies for a number of benchmark multi-agent domains. We use a uniform grammar for all the grid domains capable of capturing the domain representations. We introduce the possibility to use actions to reduce the number of features used to distinguish one transition from another, in case the action space does not change from one instance of the domain to another. In case the action space changes from one instance to another we simply ignore the actions used. We make use of the minimax algorithm to extend the meaning of an unsolvable state, ensuring that the agent wins from all states where the policy is defined, regardless of what the other agents do. These policies can be interpreted qualitatively, explained, and it can be shown mathematically that they will solve all possible instances within a class of problems. This is an advantage when making manual policy changes.

\section{Introduction}

Generalized planning is concerned with the computation of general policies or plans that solve multiple instances of a given planning domain all at once~\cite{srivastava2008network, ramirez2009plan, belle2016foundations, aguas2016generalized}. If the instances share the same strategy, it doesn't matter how different they are. For instance, in an environment where an agent has to pick up a key and then go to the door and it will open, the strategy is to approach the key, pick it up, and then approach the door in order to open it. In this example it does not matter how large the environment is and how many actions the agent has to take, since the strategy does not change.

The key question in generalized planning is how to represent and compute such general plans from the domain representation. This was solved by~\cite{frances2021learning}, for single agent settings and deterministic domains, where one agent cannot control what the other agents will do during their turn. We take inspiration of~\cite{frances2021learning} to create an approach that is capable of deal with multi-agent domains where the agent can take actions that no matter what happens in the environment the agent can win. For instance, if we are in a domain where an agent must chase an adversary, and the adversary can run slower than the agent, the strategy will be to decrease the distance between them until the agent can catch him. We know this is a winning strategy where it does not matter what the adversary does, the agent will win

In one of the most general formulations, general policies are obtained from an abstract planning model expressed as a qualitative numerical planning problem or QNP~\cite{srivastava2008network}. A QNP is a standard STRIPS planning model~\cite{fikes1971strips, bylander1994computational} extended with non-negative numerical variables that can be decreased or increased “qualitatively”; i.e., by uncertain positive amounts, short of making the variables negative. Unlike standard planning with numerical variables~\cite{helmert2002decidability}, QNP planning is decidable, and QNPs can be compiled in polynomial time into fully observable non-deterministic (FOND) problems~\cite{bonet2020high}.

The main advantage of the formulation of generalized planning based on QNPs is that it applies to standard relational domains where the pool of (ground) actions change from instance to instance. For instance, given a domain that is a grid where the actions are clicking a cell given some coordinates, the number of actions would be equal to the number of cells in the grid, then in this case the number of possible actions is different depending on the size and shape of the grid. On the other hand, while the planning domain is assumed to be given, the QNP abstraction is not, and hence it has to be written by hand or learned. This is the approach of~\cite{DBLP:conf/aaai/BonetFG19} where generalized plans are obtained by learning the QNP abstraction from the domain representation and sample plans, and then solving the abstraction with a QNP planner.


In our approach there is also the possibility to take advantage of actions if they do not change from one instance to another. For instance if the domain is a grid where the agent has to pick an object, the actions will be "move in some direction" and "pick", then the action space do not change with the size of the grid or the number of objects to be collected. The way we take advantage of domains with a fixed action space is by distinguishing with the features only the transitions that are the product of the same action, the mathematical expression of this effect is seen in the encoding section. To put it intuitively, let $tx=(s, a, s')$ be a transition from state $s$ to state $s'$ by executing the action $a$ and a transition $tx'=(t, b, t')$. If $a=b$ in a fixed action space, and $tx$ is a \emph{good} transition and $tx'$ is a \emph{bad} transition, we want to capture what change has occurred in the features. On the other hand, if $a$ is different from $b$ in a fixed action space we do not need to differentiate this transition.


We take inspiration from~\cite{frances2021learning}, but introduce an alternative approach for computing general policies in multi-agent settings that is simpler. The learning problem is cast as a self-supervised classification problem where (1) a pool of features is automatically generated from a uniform grammar applied to the domain without any domain knowledge, and (2) a small subset of features is sought for separating “good” from “bad” state transitions, and goals from non-goals. The problems of finding the final features while labeling the transitions as “good” or “bad” are addressed jointly as a single combinatorial optimization task solved with a Weighted Max-SAT solver. The approach yields general policies for a number of benchmark domains. One of the advantages of having explainable policies is that it gives the engineer the possibility to modify them easily if is necessary. For instance, if there are known exceptions within the domain.

The domain knowledge entered will come from the grammar used, which depending on the domains can be uniform. For instance, in this work we use domains based on grid worlds, which allow us to have a uniform grammar for all of them, having predicates for the value of the cells, and the relative position of some cells to others. Once the initial feature pool is automatically generated from a grammar, we do a pre-pruning of these features based on their denotation, without introducing domain knowledge. Of course, this pre-pruning is not sound, however we have observed that features with the same denotation often contain the same domain information. After pre-pruning the feature pool, a small set of features is selected automatically based on the encoding for representing the policy. The encogin guarantee that the number of features selected for representing the policy will be as small and with as little complexity as possible.

The approach is learning a model. The general policies obtained are made up of one or more rules, these rules have the form of "if a condition is met in certain features it is good to take actions that change the features in this way". This is completely different from the most common approaches, as it is not necessary to have any explicit action for the policy to be defined. However, if there is a fixed set of actions the approach is able to take advantage of this fact. Moreover, this way of using the features to represent the policy allows us to generalise to larger and more complex instances, since we are capturing the strategy that solves the problem.

In~\cite{frances2021learning}, an \emph{unsolvabel} state is defined as a state in which the agent has lost. But in multi-agent games the agent may find himself in a state where he has not lost and yet cannot win in any way if the adversary or adversaries act correctly. Therefore, we also make use of the minimax algorithm to extend the meaning of an unsolvable state, ensuring that the agent wins from all states where the policy is defined, regardless of what the other agents do. The extension of unsolvable: An state is unsolvabel if some adversary is able to win regardless of what the agent does.

Inspiration has been taken from the article by~\cite{silver2020few} to choose the domains. In~\cite{silver2020few} interpretable policies were learned by expanding with expert policies and then inferring the expert policy with the target language. In other words, imitation learning was used and the policies were learned in a supervised way. In this work we learn policies for the games of~\cite{silver2020few}, such as "two pile nim", "checkmate tactic" or "chase", but in an unsupervised way.

The paper is organized as follows. We first review related work. Then we introduce a SAT encoding and the variables, where we see what the input of our algorithm consists of. We then present the learning task, the computational approach for solving it, and the experimental results. Then we describe the grammar used in all the domains. Then we describe each of the domains, and interpret the features and policies found with our approach. These explainable policies will be the output of the algorithm. Then we show how it is possible to use our framework to introduce policies or modify them after the training process. The use case is checkamate in chess in n-moves and any board size, with two kings and a queen. Finally, we show how it is possible to use the policies with robots, both in simulators and in the real world.

\subsection{Related Work}

% XRL
% Planning, and explainable reinforcement learning (XRL)

The computation of general plans from domain encodings and sample plans has been addressed in a number of works~\cite{cui2019stochastic, fern2003approximate, silver2020pddlgym}. Generalized planning has also been formulated as a problem in first-order logic~\cite{srivastava2011environment, on2019symbolic}, and general plans over finite horizons have been derived using first-order regression~\cite{boutilier2001symbolic, wang2008first, sanner2009practical}. More recently, general policies for planning have been learned from PDDL domains and sample plans using deep learning~\cite{toyer2018action, gargmausam}. Deep reinforcement learning methods~\cite{mnih2015human} have also been used to generate general policies from images without assuming prior symbolic knowledge~\cite{groshev2018learning,hui2020babyai}, in certain cases accounting for objects and relations through the use of suitable architectures~\cite{garnelo2019reconciling}. Our work  is closest to the works of~\cite{frances2021learning}. The first provides a model-based approach to generalized planning where an abstract QNP model is learned from the domain representation and sample instances and plans, which is then solved by a QNP planner~\cite{bonet2020general}. The second learns a generalized value function in an unsupervised manner, under the assumption that this function is linear. Model-based approaches have an advantage over inductive approaches that learn generalized plans; like logical approaches, they guarantee that the resulting policies (conclusions) are correct provided that the model (set of premises) is correct. The approach developed in this work does not make use of QNPs or planners but inherits these formal properties. The problem of generalisation in adversarial domains has been studied recently in~\cite{zhang2021understanding, subramani2020learning} from the perspective of reinforcement learning, trying to address the problem of transferability.


\section{Contribution: Generalized Planning in multi-agent settings}

A key question in generalized planning is how to represent general plans or policies when the different instances to be solved have different sets of objects and ground actions. We take inspiration of the work of~\cite{frances2021learning} and we extend it to multi-agent domains. One solution is to work with general features (functions) that have well defined values over any state of any possible domain instance, and think of general policies $\pi$ as mappings from feature valuations into abstract actions that denote changes in the feature values~\cite{bandres2018planning}. When a transition is taken by the agent the environment or other agent changes state, so when we record the AND/OR graph of states, capturing how the environment or the other agent can change each state. In this work, we use abstract actions as~\cite{frances2021learning}, in addition to the possibility of taking advantage of using an action space fixed for all instances in the domain.  The approach is SAT-based, so first we will define the variables and encoding, and then how we learn general policies.


\section{Action Space}

In the work of~\cite{frances2021learning} it was assumed that the set of actions was not fixed in all instances and abstruse actions were always used. In cases where the action set was fixed across all instances this information was simply ignored. There are some domains where it is useful to take advantage of fixed action sets, where in a certain abstract state it will always be good to do a certain action. We have implemented two ways to take advantage of a fixed set of actions. (1) Have policies where we reason about the value of features in a state, and whether at that value it is good to take an action $a$. (2) Have policies where we take into account the fixed set of actions and reason about the change of feature values in a transition, to put the two concepts together we only differentiate transitions where the actions are the same.


\subsection{Features}

Let us have a pool of features $f$. Where $f$ is a function that has as input a state and as output a numerical value. The initial feature pool is produced using the logical description language, i.e., genereted from a grammar. In addition, a parameter is used to determine the maximum complexity of the features. Once the initial pool of features is produced, a pruning is performed based on the denotation of each feature with the following philosophy: if two features, $f$ and $f'$ have the same denotation in all states, it is likely that they are capturing the same information of the domain, therefore we keep the simpler of the two features. With these features we can basically create abstract states, or representations, the challenge will be to find which ones are useful to represent the strategy that solves the problem. After the feature pruning, the set of features $f$ that are left represent $F$, and it is the one that will have the SAT solver as input. Based on the encoding the SAT solver will select the features of $F$ needed to represent the policy, trying to minimize the complexity and the number of features used.


\subsection{SAT Encoding}

Our encoding is parametrized by
\begin{itemize}
 \item Pool $F$ of description logic features $f$, each with given feature complexity $\mathcal{K}(f)$.
 \item A (training) set $T$ of transitions $(s, a, s') \in T$ from a number of instances of the same domain.
       We assume $T$ is \emph{closed}: if $s$ appears in the first position of a transition, then all possible
       transitions starting in $s$ in that instance appear in $T$ too.
 \item A set $adv(s)$ of all possible moves by the adversary~\footnote{Second agent that plays the game} on state $s$.

 \item For each state $s$ appearing in some transition, full information on whether $s$
       is a goal, unsolvable, or alive.
       Also access to the minimum distance-to-goal $V^*(s)$ for each state $s$.
 \item A parameter $\delta$ which is a ``slack'' value to determine the maximum deviation from the optimal $V^*(s)$
 what we will allow in our policy. This will be made clearer in the encoding below.

 \item A Boolean parameter \texttt{allow\_bad\_states} with intended meaning ``allow the policy
        to remain undefined for some of the states in the training set''.
 \item A parameter \texttt{action\_labels} with possible values:
 \begin{itemize}
   \item \texttt{ignore}: Don't exploit the info on what action induced what transition. This is necessary
  when the set of actions changes over different instances of the domain.
   \item \texttt{use$_1$}: Use info on action labels, reasoning only about whether taking an action in a certain
   (abstract) state is good or not. This assumes that the action set is fixed over the entire domain.
   \item \texttt{use$_2$}: As above, but reason too about the feature change. E.g. your policy can have rules as
  ``When $f>0$, take action $a$ \emph{only if $f'$ increases}.
 \end{itemize}
\end{itemize}


\subsubsection{Main Variables}

\begin{itemize}
 \item $Good(s, a, s')$, for $(s, a, s') \in T$ with $s$ being alive.

 \item $Bad(s)$ for $s$ an alive state appearing in $T$.

 %\item $Good(s, s')$ for $s$ alive, $s'$ solvable and $(s, s') \not\in \badtx$.

 \item $V(s, d)$ for $s$ alive, and $d \in [0, D]$, where $D = \max_{s} \delta \cdot V^*(s)$,
 with intended denotation $V(s)=d$.
 Note that for states $s$ that are a goal, we know $V(s)=0$,
 and for states $s$ that are unsolvable, we know that $V(s) \neq d$ for all $d$.
 Thus, we can restrict SAT variables $V(s, d)$ to those states $s$ that are alive.

 \item $Select(f)$, for each feature $f$ in the feature pool.
\end{itemize}


\subsubsection{Terminology}

A state is called \emph{reachable} if there is a path to it from $s_0$, and it is called \emph{solvable} if there is a path from it to a goal state. A state is \emph{alive} if it is solvable, reachable and not a goal state~\cite{frances-et-al-ijcai2019}. A state is \emph{unsolvable} if the agent has lost. We use $T$ to denote the set of all transitions $(s, a, s')$ in the training sample such that $s$ is alive.

\subsubsection{Extension of unsolvaility}

In some domains the agent can win from any state of the problem except for some specific states where the agent has lost. In other more complex domains, there are states where the agent has not lost, however, the success of the agent depends on whether the opponent plays well or not. In other words, if the opponent plays correctly the agent has no way to win.

With our approach we want policies that are always able to win, no matter what state the agent is in. Therefore, we consider states where the agent can lose no matter what he does as \emph{unsolvable}. To classify unsolvable states we use the minimax algorithm.

\subsubsection{Hard Constraints}

The constraints recognize good state transitions, the distance from an state to the goal, and distinguish good and bad states.

\smallpar{C1 (if $\neg$\texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
\bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}


\smallpar{C1 (if \texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
Bad(t) \lor \bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}


\smallpar{C2} $V$ is always descending along Good actions, \textbf{taking into account all possible adversary responses}. For each $(s, a, s')$ in the set of non-deterministic transitions such that both $s$ and $s'$ are alive, where all possible $s''$ are alive, where $s''$ is the result of the adversary transforming $s'$ into $s''$ by executing $a'$, post:
%Good(s, a) \land V(s, k) \rightarrow \bigvee_{1 \leq k' \leq k} V(s', k'),&\;\; \text{for } k \in [1, D].
\begin{align*}
Good(s, a, s') \rightarrow V(s'') < V(s)  \;\; \text{for } (s, a, s') \in T,  s'' \in adv(s').
\end{align*}

%\smallpar{C3} All descending transitions must be considered Good:
%\begin{align*}
% V(s, d) \land V(s'', d') \rightarrow Good(s, a),&\;\; \text{for $s$ alive, $(s, a, s'') \in T$, $1 \leq d' < d \leq D$,} \\
% Good(s, a),&\;\; \text{for $s$ alive, $s'$ goal.} \tag{\theequation${}^\prime$}
%\end{align*}

%\smallpar{C4-5} Any upper bound on $V(s)$ (for $s$ not a goal) needs to be justified:
%\begin{align*}
% V(s) \leq d+1 \rightarrow \bigvee_{\substack{
% s' \text{ goal child of } s\\
% (s, s') \not\in \badtx}} Good(s, s') \lor
% \bigvee_{\substack{
% s' \text{ alive child of } s\\
% (s, s') \not\in \badtx}} GV(s, s', d),&
% \;\; \text{for $s$ alive, $d \in [0, D)$.} \\
% \neg V(s) \leq 0,&\;\; \text{for $s$ not a goal.}
%\end{align*}


\smallpar{C3}
Variables $V(s, d)$ define a function that is total over the set of alive states,
and such that $V(s)$ is within lower bound $V^*(s)$ and upper bound $\delta \cdot V^*(s)$:
\begin{align*}
 \bigvee_{V^*(s) \leq d \leq \delta \cdot V^*(s)} V(s,d),&\;\; \text{for $s$ alive.} \\
 \neg V(s, d) \lor \neg V(s, d')&\;\; \text{for $s$ alive, $1 \leq d < d' \leq D$.}
\end{align*}


\smallpar{C4}
Good transitions can be distinguished from bad transitions.
%Let $(s, s')$ and $(t, t')$ be \emph{representative} transitions
%of two different equivalence classes such that $(s, s') \not\in \badtx$
%(which implies that $s'$ is solvable). Then,
Let $(s, a, s')$ and $(t, b, t')$ be \emph{representative} transitions
of two different equivalence classes such with $s$, $t$ alive. Then,
\begin{align*}
 Good(s, a, s') \land \neg Good(t, b, t') \rightarrow
 D2(s, s', t, t'),&\;\; \text{  }
% Good(s, a, s') \rightarrow
% D2(s, s', t, t'),&\;\; \text{for $s, t$ alive, $(t, t') \in \badtx$.}
\end{align*}


\noindent where $D2(s, s', t, t')$ is shorthand for $\bigvee_{} Select(f)$, with $f$ ranging over:
\begin{itemize}
 \item[(a)] all features that distinguish state $s$ from $t$, if \texttt{action\_labels = use$_1$}.
 \item[(b)] all features that distinguish $s$ from $t$ \textbf{plus} all features that
 change differently along $(s, s')$ than along $(t, t')$.
\end{itemize}
Additionally, if \texttt{action\_labels = use$_1$} or \texttt{action\_labels = use$_2$}, then
\textbf{the above constraint is only posted for transitions such that $a=b$}.


\smallpar{C5 (Optional)}
Goals are distinguishable from non-goals.
\begin{align*}
\bigvee_{f \in D1(s, s')} Select(f),&\;\; \text{for $s$ goal, $s$ not a goal}
\end{align*}


% GFM: I'm commenting this out, I don't think this is needed at all, actually I don't even know where this is coming from,
% maybe some old draft???
%\smallpar{C10 (Optional)}
%All selected features need to have some Good transition that takes them to $0$:
%\begin{align*}
% Selected(f) \rightarrow \bigvee_{(s, a, s') \in Z(f)} Good(s, s'),&\;\; \text{for $f$ in pool}
%\end{align*}
%
%\noindent where $Z(f)$ is the set transitions starting in an alive state that change the denotation of $f$ from something
%larger than 0 to 0.


\subsubsection{Soft Constraints}

\smallpar{C6}
A constraint $\neg Select(f)$ for each feature $f$ in the pool, with weight equal to its complexity $\mathcal{K}(f)$.

\smallpar{C7 (if \texttt{allow\_bad\_states})}
A constraint $\neg Bad(s)$ for each alive state $s$, with weight equal to one.


\subsection{Constraints Description}

How we arrive at these constrains has to do with the observation of multi-agent domains and winning strategies. We observed that the dilemma of adversarial games meant that we were confronted with the fact that decisions were not made by a single agent, and we had to take into account that there were also other agents trying to "win". Moreover, this meant that in many domains we were faced with scalability problems.

During policy learning, it does not matter which agents have been played against to learn the policy, as all their possible actions in each state will be taken into account when expanding the state space.

We observed that in many domains the set of actions is fixed for all agents. In the paper~\cite{frances2021learning} there were $Good(s, a, s')$ variables. But these were not able to take advantage of fixed action sets, so we decided to include the information of the actions in these domains to alleviate scalability problems, for that reason we have variables such as $Good(s, a, s')$.
 This allows us to reduce the number of C4 clauses.

It was assumend by~\cite{frances2021learning} assumed that it was possible to get from any state of the problem to the objective, without entering dead ends (states where the agent can lose, independently of what he does). However, in many multi-agent domains there are states where the agent has not lost, but no longer has the possibility of winning in that state no matter what the agent does. That is why we included the $Bad(s)$ variables, so that there could be states without any actions that lead to a path where the agent always wins. However, in our approach, if from a certain state there is a strategy that can be represented with the logic language and that always wins over the rest of the agents, we guarantee that we will find it. However, allowing the use of $Bad(s)$ variables is optional, through~$\texttt{allow\_bad\_states}$, since there are domains where we know that there is a winning strategy from any state.

In C1 it can be understood that each state must have a possible transition that is good, if ~$\neg\texttt{allow\_bad\_states}$. This makes sense for many of the domains, as we want our overall strategy to be able to decide which transition to take in any given state. We then design counterin C2 to ensure that the strategy takes paths that do not fall into cycles by introducing the adversarial factor. To introduce the adversarial factor into the strategy we make sure that the potential $V$ of the state $s$ is greater than all the potentials $V$ of all the possible $s''$. The possible $s''$ are the result of the agents or the environment executing an action $a'$ on $s'$, where $s'$ is the state the agent reaches after executing $a$ on $s$. Note that adversaries will often have multiple $a'$ actions to execute on $s'$, which will often result in a different set of $s''$.

\pagebreak


%\subsection{Optimizations}


%\paragraph{Non-distinguishability of transitions as an equivalence relation.}
%Any fixed, given pool of features $F$ implicitly defines an equivalence relation where to transitions are
%equivalent iff they cannot be distinguished \emph{by any feature in $F$}.
%If two transitions cannot be distinguished by any feature, then clearly either the policy computed by the SAT solver
%considers all of them as ``good'', or as ``bad''.
%We'll exploit this by using one single SAT variable to denote whether \emph{all transitions in a given equivalence
%class} are good or bad. When exploiting this notion of equivalence (which is implemented as an optional feature of
%the CNF generator), then every mention below to SAT variable $Good(s, a, s')$ needs to be read as $Good(s_{\star}, s_{\star}')$,
%where $(s_{\star}, a, s_{\star}')$ is the \emph{representative} transition of the equivalence class to which $(s, a, s')$ belongs.


%\paragraph{``Bad'' transitions.}
%We use \badtx{} to denote the set of transitions that have been determined at preprocessing as necessarily
%\emph{not} good.
%At the moment, this set contains all transitions that go from an alive to an unsolvable state and, if using
%equivalence relations, all those other transitions in whose equivalence class there is some other ``bad'' transition.


\section{Motivating Example}

We will now consider a grid-world in which the agent has to reach a goal. The grid-world is made of cell, organized in rows and columns. The sorts used are: $cell$, $row$, $column$ and . The constants for the sort $cell$ have the form $cell.r\_c$, this constant refers to the cell that is at the same time in the row $r$ and in the column $c$. The constants for sort $row$ are $row.r$, and this constant refers to the row $r$. The constants for sort $column$ are $column.c$, and this constant refers to column $c$. Each sort can be linked to an object, using a one-arity predicate, depending on the objects of the domain, given by a predicate of arity one with any variable belonging to the sorts $cell$, $row$ or $column$. An example for the sort $cell$ would be $cell\_has\_v(cell)$, where the variable of type $cell$ can take the value of $cell.r\_c$, and this predicate would indicate that the cell $cell.r\_c$ has an object with value $v$, which is the name of the object currently in $cell.r\_c$. To map the relative position between constants of the same sort, we use two-arity predicates, which have as variables these constants belonging to the same sort. For example, $adjacent\_cell(cell, cell)$ takes two constants of the sort $cell$, and means that the two cells are adjacent. The same philosophy would apply to the predicates $adjacent\_col(column, column)$ or $adjacent\_row(row, row)$. $cell\_has\_v()$ and $adjacent\_row/col()$ represent domain knowledge that needs to be encoded at design time.

\section{Using the algorithm}

In this section we describe the end-to-end process of the steps necessary to use the algorithm~\footnote{The code of our project: ~\url{https://github.com/iOrb/gpl}}. To run it we must first have a simulator of the domain in which we can execute an action and see the next state. We also have to take into account that we want to expand the whole state space, so we have to know to which states the adveraries can go.

\begin{enumerate}
  \item First we must decide which predicates we are going to use, e.g., $cell\_has\_v()$ . Then we will have to define these predicates programmatically, we have done it with Tarski~\footnote{Tarski is a framework for the specification, modeling and manipulation of AI planning problems. Code:~\url{https://github.com/aig-upf/tarski}}. This defines our language.

  \item Then we need to be able to transform any state of the problem into a set of atoms taking into account our predicates.

  \item Finally, we define the experiment, introducing the configuration parameters of the simulation environment if any. We can also modify the parameters of our approach, for example the maximum complexity of the features, which are generated incrementally based on the defined language. And we are ready to execute it, obtain the policy and test it in diferent instances.
\end{enumerate}


\newpage

\section{Empirical Results}

In all domains we managed to solve with the shown policies any instance of the problem class, of any size. The policies have been found by learning unsupervised form from the tranining instances shown in the domains.

The sections broken down into each domain are: Description of the domain, the training instances used to discover the policy, the learned features, description of the features, learned policy, description of the policy, and finally an example of a possible game, from initial state to goal, using the discovered policy in a test instance against an expert adversary. The initial state of the example test instance is taken from the infinite set of possible instances in the domain.

Examples of games in which the learned policies are executed against other agents can be found in the appendix of the article.

Layouts have been created to show the problems and examples of game traces. The objects that can be found in the layouts are equivalent to:
\begin{Verbatim}[fontsize=\footnotesize]
  A: Agent
  D: destination
  T: adversary
  K: white-king
  Q: white-queen
  R: white-rook
  k: black-king
  m: martian
  p: pet
  t: token
\end{Verbatim}

\subsection{Space Invaders}

\subsubsection{Description}
This domain is a simplified version of the original Space Invaders in which the Martians cannot shoot and cannot move below the penultimate row. In this domain the agent can $shoot$, move to the $right$ or to the $left$. The agent during his turn can do two actions, while the Martians can do one. Note that if the Martians and the agent have only one action per turn the agent cannot win from all possible states in the game, this is because of the situation where the agent stands in the Martian's column to shoot him on his next turn, but the next turn (the Martian's turn) the Martian can move away, this leads to an infinite loop if both are playing perfectly. That is, the agent must be able to move to the agent's column and then shoot. To win the agent has to kill all the martians, for this he must shoot each of them. The agent kills an opponent if he shoots while being in the same column, and he can only kill one for each shoot. The martians has three possible actions $left$, $right$, and $down$. All the adversaries move in the same direction. The agent cannot be destroyed in any way by the Martians, they can go down to the row above the agent. The agent cannot collide with the Martians as they cannot descend to the row above it. Note that the policy found is optimal and will solve all possible instances of the game. The policy is also optimal for a version of the game where the Martians destroy the agent when they reach the row above him.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # m  m  .  m #
  # m  m  .  m #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  A  . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]

(A) Num[cell-has-martian] [k=1]
(B) Dist[col-has-agent;adjacent_col;col-has-martian] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of martians\\
$B$. Minimum absolute horizontal distance between a column with an agent and a column with a martian

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B>0 -> {A ·, B ↓}
  2. A>0 AND B=0 -> {A ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the first rule if the agent is not in the column of a Martian, $B$ is greater than zero, it is good to decrement $B$, i.e. move agent towards the closest column with martians. The second rule is applicable when the agent is under a Martian, in the same column, and according to this rule it is good to decrease the number of Martians, i.e. execute the action $shoot$.

\subsection{Chase}
\subsubsection{Description}
In this domain an agent has to chase an adversary. The goal is defined by the agent being in a adversary's adjacent cell. The set of possile actions for the adversary is ~\{$up$, $right$, $down$, $left\}$. The set of possile actions for the agent is ~\{$rightup$, $rightdown$, $leftup$, $leftdown\}$. The agent and the adversary can move only once per turn.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
#. . . . . .#
#. A . . . .#
#. . . . . .#
#. . . . T .#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Dist[col-has-agent;adjacent_col;col-has-adversary] [k=4]
  (B) Dist[row-has-agent;adjacent_row;row-has-adversary] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Absolute vertical distance between the agent and the adversary\\
$B$. Absolute horizontal distance between the agent and the adversary
\subsubsection{Policy}

\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B>0 -> {A ↓, B ↓}
  2. A=0 AND B>0 -> {A ↑, B ↓}
  3. A>0 AND B=0 -> {A ↓, B ↑}
\end{Verbatim}

\subsubsection{Policy Description}
According to rule one, is good to decrease the horizontal and vertical distance from the angent to the adversary if possible, i.e. move in diagonal towards him. Following the rule two if the agent is in the column of the adversary it is good to decrease the vertical distance. Finally, following the rule three if the agent is in the row of the adversary it is good to decrease the horizonal distance.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
 # A..... # ...... # ...... # ...... # ...... # ...... # .....T # ....AT #
 # ...... # .A.... # .A.... # ...... # .....T # ...A.T # ...A.. # ...... #
 # .....T # .....T # ...... # ...... # ...... # ...... # ...... # ...... #
 # ...... # ...... # .....T # ..A..T # ..A... # ...... # ...... # ...... #
\end{Verbatim}

\subsection{Nim}
\subsubsection{Description}
This is the Nim version used in~\cite{silver2020few}.In this domain there are two stacks (columns) of tokens. The number of actions available in each state depends on the number of tokens, a valid action involves removing one of the tokens. When a token is removed, that token and all the tokens above it are removed as well. When a token is removed, the turn is changed and the other player must remove a token as an action if there is one. The objective of the game is to leave the opponent with no actions available on his turn, i.e. without any token. The agent and the adversary has one action per turn.
It is interesting to note that in this game not all states are solvable if the opponent plays optimally. Knowing that the expert policy is to leave the opponent with both stacks with the same number of tokens, this state with both stacks leveled will not be solvable. Therefore, in order to solve it, we have made use of the $Bad(state)$ feature, which allows us not to have to define a good transition for each state, and as an alternative to label it as a bad state. The advantage is that if the initial state is solvable, bad states can always be avoided. However the policy is not complete for all states, in particular there will be no good transition for states with the two token stacks leveled, as we will see below, which we can say is the optimal representation of the game strategy. The winning tactic in this game is to level the two stacks of tokens by removing the token next to an empty cell and diagonally above another token, como se puede ver en~\cite{silver2020few}. The intuition behind this tactic is that if we follow it we can always reach a final situation where the opponent is forced to remove all the tokens from one pile, then we can remove the remaining tokens from the other pile.


\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  . #
  # t  . #
  # t  . #
  # t  t #
  # t  t #
  # t  t #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Num[And(cell-has-token,Exists(left_cell,cell-has-token))] [k=5]
  (B) Num[And(cell-has-token,Forall(leftdown_cell,cell-has-token))] [k=5]
  (C) Num[And(cell-has-token,Forall(rightdown_cell,cell-has-token))] [k=5]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of tokens with a token immediately to the left of it\\
$B$. Number of tokens s.t. any cell to the leftdown also has a token.\\
$C$. Number of tokens s.t. any cell to the rightdown also has a token.

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. B>0 AND C>0 -> {A ·, B ↓, C ↓}
\end{Verbatim}

\subsubsection{Policy Description}
The policy guarantees to leave both stacks with the same number of tokens. This is done by decreasing the number of tokens s.t. any cell to the leftdown and to the rightdown has a token, without changing the number of tokens with a token at left to them. The only way to fulfill this policy is to remove the token that has a token to its leftdown and an empty cell to its left, or to remove the token that has a token to its rightdown and an empty cell to its right.

As the policy clearly shows, there is no good transition, that includes a change in the number of tokens that a token has on its left. In other words, states where the same number of tokens in the two stacks cannot be preserved with some legal move are bad, and therefore they have no good transition. In fact according to the policy it is good to leave the opponent in these states bad, because there will be no legal move for the opponent to leave us with the same number of tokens in the two stacks.


\subsection{Delivery}
\subsubsection{Description}

In this domain the agent has to pick up a missed pet and take it to a certain location, the destination. The pet has the option on its turn to escape from its captor to any of the cells adjacent to the pet, even if is holded by the agent it can escape. The goal is defined by the pet being droped at the destination cell, not holded by the agent. In this domain we use the additional predicate $holding_pet$ to indicate that the agent is holding the pet. The agent has two actions per turn and the adversary one, otherwise the game would not be solvable for all the states. The set of possile actions for the agent is ~\{$pick$, $drop$, $rightup$, $reightdown$, $leftup$, $leftdown\}$. The agent, in order to pick up the pet has to execute the action $pick$ adjacent to the the pet's cell. The agent, in order to drop the pet has to execute the action $drop$ while $holding_pet$, and the pet will be droped at de agent's cell. The set of possile actions for the pet consist in escape to one of the adjacent cells to it, i.e. at most eight possible actions depending on the position of the pet in the grid. The pet can not move into the delivery point.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  .  .  .  .  A #
  # .  D  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  p  .  .  . #
  # .  .  .  .  .  . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Atom[holding_pet] [k=1]
  (B) Dist[cell-has-pet;adjacent_cell;cell-has-agent] [k=4]
  (C) Dist[cell-has-destiny;adjacent_cell;cell-has-agent] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. The agent is holding a pet
$B$. Manhattan distance between the agent and the pet\\
$C$. Manhattan distance between the agent and the destination

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B>0 -> {B ↓}
  2. A>0 AND C>0 -> {C ↓}
  3. A>0 AND C=0 -> {A ↓}
  4. A=0 AND B>0 -> {A ↑, B ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the first rule, it is good to move towards the pet if the agent is not holding it and the distance to it is greater than zero. According to the second rule, if the agent has the pet, and is at the destination, it is good to move towards the destination. According to the third rule of the policy, if the agent is holding the pet, and is at destination it is good to drop the pet. Finally, according to the fourth rule of the policy, if the agent does not have the pet, and is at the adjacent cell it is good pick it.

\subsection{Shoot}
\subsubsection{Description}
In this domain an agent has to shoot an adversary. The goal is defined by the agent being alone on the grid, having finished with the adversary. The agent can execute the action $shoot$, but the opponent does not, so the opponent is able to win the game from any state of any instance if he knows the right strategy. The agent and the adversary share the set of actions ~\{$up$, $right$, $down$, $left\}$. To reach the goal state, the agent has to be in the same row or column as the adverary and execute the action $shoot$. When the agent executes the $shoot$ action on the adversary's line, the adversary disappears. We can define the goal state, as the adversary not being on the board. The agent have one actions per turn and the adversary two, so that the agent can stand in the opponent's row or column and shoot before the opponent escapes.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # . . . . #
  # A . . . #
  # . . T . #
  # . . . . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[cell-has-adversary] [k=1]
  (B) Bool[And(row-has-adversary,row-has-agent)] [k=3]
  (C) Bool[And(row-has-adversary,Exists(adjacent_row,row-has-agent))] [k=5]
  (D) Dist[col-has-agent;adjacent_col;col-has-adversary] [k=4]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The adversary is in the board\\
$B$. The agent and the adversary are in the same row\\
$C$. The agent row is adjacent to the adversary row\\
$D$. Absolute horizontal distance between the agent and the adversary

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 AND D>0 -> {A ·, B ·, C ·, D ↓}
  2. A>0 AND B=0 AND C>0 AND D>0 -> {A ·, B ↑, C ↓, D ·}
  3. A>0 AND D=0 -> {A ↓}
  4. A>0 AND B>0 -> {A ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the fourth rule of the policy, if the agent is in the same column as the agent it is good to decrease $A$, i.e. shoot the adversary. In the same way, according to the third rule, if the agent is in the same row as the adversary it is good to execute the action $shoot$. According to the second rule, if the agent is in the row adjacent to the adversary it is good to get in the same row. Finally, according to the first rule, if the agent is not in the row adjacent to the adversary, and is not in the same row or column, it is good to decrease the horizontal distance between the agent and the adversary. The first rule can also be read as, it is good for the agent to approach the adversary horizontally.


\subsection{Checkmate in one with Queen}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the grid. In order to win the white pieces has to do a checkmate in one movement. The white pieces have the white-king and the white-queen, and black pieces have the black-king.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  Q #
\end{Verbatim}


\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_queen)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_king)] [k=3]
  (D) Bool[And(row-has-black_king,row-has-white_queen)] [k=3]
  (E) Bool[And(cell-has-black_king,Exists(adjacent_cell,cell-has-white_queen))] [k=5]
  (F) Bool[And(col-has-white_king,Exists(adjacent_col,col-has-white_queen))] [k=5]
  (G) Bool[And(row-has-black_king,Exists(adjacent_row,row-has-white_king))] [k=5]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The black-king and the white-king are in the same column\\
$B$. The black-king and the white-queen are in the same column\\
$C$. The black-king and the white-king are in the same row\\
$D$. The black-king and the white-king are in the same row\\
$E$. The black-king's cell and the white-queen's cell are adjacent\\
$F$. The white-king's column and the white-queen's column are adjacent\\
$G$. The black-king's column and the white-king's column are adjacent

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ·, F ·, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ↑, F ·, G ·}
  2. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ·, F ↓, G ·}
  3. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ·, F ·, G ·}
  4. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, C ·, D ·, E ↑, F ↑, G ·}
  5. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ↑, F ↓, G ·},
                                                            {A ·, B ·, C ·, D ↑, E ·, F ↓, G ·}
  6. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ↓, G ·}
  7. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, C ·, D ·, E ↑, F ·, G ·}
  8. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ·, G ·}
  9. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·}
  10. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ·, G ·}
  11. A>0 AND B>0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↓, C ·, D ↑, E ·, F ·, G ·}
\end{Verbatim}

\subsubsection{Policy description}
The policy is an abstraction of all the possible one-move checkmates that exist on a chessboard of any size with two kings and a queen. For example rule one tells us that if the white king is on the same column as the black king, it is good to put the white queen on the same row as the black king, avoiding squares where the white queen would be in a cell adjacent to the black king. Following on from the first rule, transitions where the white queen is placed on the same column as the black king, and in a cell adjacent to the black king, would also be good, i.e. it is good for the white queen to be placed between the white and black kings. This policy will generalise to any chessboard size board.

\subsubsection{Task instance distribution}
To generate all the possible states from which the queen can checkmate the black king in one, in a board of $NxM$, we follow the following procedure. We generate all the possible checkmates that can be given to the black king if it is in the first row, then we apply three rotations to each board, 90, 180 and 270 degrees. To generate the checkmate with the black king in the first row we must take into account that for each black king position in this row there are three possible positions of the white king that allow a checkmate in one. In all these possible positions of the two kings we place the king in all the squares where it can checkmate the black king, and eliminate those where it would be giving a check. For the training, one hundred random boards of five by fifty are selected. To check that the policy is complete, it is tested on all possible, for each board size $NxM$ where both $N$ and $M$ are numbers no greater than twenty randomly selected.


\subsection{Checkmate in one with Rook}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the board. In order to win the white pieces has to do a checkmate in one movement. In white pieces have the white-king and the white-rook, and black pieces have the black-king.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  R #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_rook)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_rook)] [k=3]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The black-king and the white-king are in the same column\\
$B$. The black-king and the white-rook are in the same column\\
$C$. The black-king and the white-rook are in the same row

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 -> {A ·, B ·, C ↑}
  2. A=0 AND B=0 AND C=0 -> {A ·, B ↑, C ·}
\end{Verbatim}

\subsubsection{Policy description}
The policy is an abstraction of all the possible one-move checkmates that exist on a chessboard of any size with two kings and a tower. According to rule one, if the black king is in the same column as the white king, it is good to put the white rook in the same row as the white king. According to the second rule if the black king is not on the same column as the white king, it is good to put the white rook on the same column as the white king. This policy will generalise to any chessboard size board where checkmate is possible in one with rook and king.

\subsubsection{Task instance distribution}
To generate all the possible states from which the rook can checkmate the black king in one, in a board of $NxM$, we follow the following procedure. We generate all the possible checkmates that can be given to the black king if it is in the first row, then we apply three rotations to each board, 90, 180 and 270 degrees. To generate the checkmate with the black king in the first row we must take into account that for each black king position in this row there are one possible position of the white king that allow a checkmate in one. In all these possible positions of the two kings we place the king in all the squares where it can checkmate the black king, and eliminate those where it would be giving a check. For the training, one hundred random boards of five by fifty are selected. To check that the policy is complete, it is tested on all possible, for each board size $NxM$ where both $N$ and $M$ are numbers no greater than twenty randomly selected.


%\subsection{Checkmate in n with Rook}
%\subsubsection{Description}


%\subsubsection{Training instances}
%\begin{Verbatim}[fontsize=\footnotesize]
%  # .  k  .  .  K #
%  # .  .  .  .  . #
%  # .  .  .  .  . #
%  # R  .  .  .  . #
%  # .  .  .  .  . #
%\end{Verbatim}


%\subsubsection{Features}
%\begin{Verbatim}[fontsize=\footnotesize]
%  (CM) Atom[checkmate] [k=1]
%  (SM) Atom[stalemate] [k=1]
%  (QbK) Atom[white_rook_between_kings] [k=1]
%  (KE) Atom[black_king_closed_in_edge_by_white_rook] [k=1]
%  (KD) Dist[cell-has-white_king;adjacent_cell;cell-has-black_king] [k=4]
%  (BKhA) Bool[And(cell-attaked-by-black_king,Not(cell-attaked-by-white))] [k=4]
%  (A) Num[And(cell-attaked-by-black_king,Not(cell-attaked-by-white_rook))] [k=4]
%  (WQadjBK_row) Bool[And(row-has-white_rook,Exists(adjacent_row,row-has-black_king)))] [k=6]
%  (WQadjBK_col) Bool[And(col-has-white_rook,Exists(adjacent_col,col-has-black_king)))] [k=6]
%  (WQadjBK) Bool[And(cell-has-white_rook,Exists(adjacent_cell,cell-has-black_king)))] [k=6]
%  (WQadjWK) Bool[And(cell-has-white_rook,Exists(adjacent_cell,cell-has-white_king)))] [k=6]
%\end{Verbatim}


%\subsubsection{All Transition Features}
%This features reason about how the value of the features can change in all possible successors.
%\begin{Verbatim}[fontsize=\footnotesize]
%  (CA) Bool[can(A, DEC)] [k=2]
%  (CCM) bool[can(CM, INC)] [k=2]
%\end{Verbatim}


%\subsubsection{Feature Description}
%$CM$. Checkmate\\
%$SM$. Stalemate\\
%$KE$. Black king is closed in an edge of the table by the white queen\\
%$QbK$. White queen is somewhere between kings, rows or columns\\
%$A$. The number of cells attaked by black king and not attacked by white queen\\
%$BKhA$. Exist a cell attaked by Black king and no attacked by white pieces\\
%$Qaw$. White queen is attaked and without protection, i.e. $WQadjBK$ AND $WQadjWK$\\
%$WQadjBK_line$. White queen is in an adjacent line to the black king, i.e. $WQadjBK_row$ OR $WQadjWK_col$\\
%$CA$. It is possible to decrease the number of cells attaked by black king and not attacked by white queen\\
%$CCM$. It is possible to do checkmate in the current state


%\subsubsection{Policy}
%  \begin{Verbatim}[fontsize=\footnotesize]
%   1. KE=0, CA>0 -> {A ↓}
%   2. KE=0, CA=0 -> {A ·, KD ·}
%   3. BKhA=0, WQadjBK_line>0 -> {KD ·, WQadjBK_line ·}
%   4. CCM=0, KE>0 -> {KD ↓}
%   5. CCM=1 -> {CM ↑}
%  \end{Verbatim}


%\subsubsection{Policy Implicits}
%The next features has to be in that certain value after the transition for classifing the transition as good, taking into account the policy rules.
%  \begin{Verbatim}[fontsize=\footnotesize]
%   Qaw=0, Qbk>0, SM=0
%  \end{Verbatim}


%\subsubsection{Policy description}
%Rules 1 and 2 cover the cases in which the king is not enclosed in an edge of the file. The first rule states that it is good to take transitions that decrease the number of squares available to the black king by using the queen. The second rule says that if the first rule is not applicable it is good to take transitions that do not alter the number of squares available to the black king, without moving the white king. The third rule refers to those cases where the black king had moved to its only free square, and classifies as good those transitions in which the queen is moved so that it is not stalemate and remains on the line adjacent to the black king. Rule four covers cases in which the black king is locked on an edge by the queen, and it is good to move the white king closer. finally, rule 5 says that if it is possible to checkmate it is good to do so. It should be noted that thanks to the implicit parts of the policy the king will never be left tied up and unprotected, stalemate will never occur and the white king will always be placed and remain in a line that is between the lines of the kings, whether rows or columns.


%\subsubsection{Example of Game}
%\begin{Verbatim}[fontsize=\footnotesize]
%  K.... # K.... # K.... # ..... # ..... # ..... # ..... # ..... # ..... #
%  ..Q.. # ..... # ..... # ..K.. # ..K.. # ..K.. # ..... # ..... # ..... #
%  ..... # ..... # ..... # ..... # ..... # ..... # ..K.. # ..K.. # ..K.. #
%  ..... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # ...Q. #
%  ....k # ....k # ...k. # ...k. # ..k.. # ...k. # ...k. # ....k # ....k #
%\end{Verbatim}

%\subsection{Dealing with real robots}

%First it is necessary to have a simulator of the robot's environment discretised. The policy would be learned with the algorithm described in the paper without involving the robot. This policy is learned from a single instance in the simulator, and can then be run on different and larger instances of the domain. Once the policy is learned with the simulator and the algorithm, it is only necessary to translate the state where the robot is into a state that can be executed by the simulator. Then a transition is selected as good using the already learned policy, and the policy returns the action executed in the simulator to take that transition. The robot controller must be able to handle executing these actions in the real world. This process is repeated until the goal is reached.

%So, to be able to execute the policies on real robots, it is only necessary to have a simulator and an algorithm to translate the state of the robot into a state executable by the simulator.

%For example, with a turtle robot that has to pick up a certain number of packages and take them to a certain place. We only need a grid sifmulator, where the objects "package", "agent" and "destination" exist. The simulator must be able to execute the actions to move the robot, pick and drop. The strategy that we would learn with the simulator and our algorithm would be the same as the one learned in the "Delivery" domain, in the results section. The robot controller should be able to cope with executing the actions output by the algorithm, and ideally end up in the state that the simulator expects.

%Of course, you could give the initial state translated to the simulator and have the learned policy execute a sequence of actions up to the goal. But this method makes us more vulnerable to the noise that the real world produces.

\subsection{Noisy adversaries}

Notice that for the algorithm to work it must be possible to win from the initial state of the problem no matter what the opponent does. So in domains where the agent has two actions per turn, which are "Space Invader" and "Delivery". If both players have one action per turn the agent's strategy does not change. Therefore in some domains the learned policy can deal with a noisy adversary even though it has not been learned by playing against him.

\subsection{Conclusions}

It is possible to learn policies unsupervised in small instances and transfer them to larger instances in the same domain. We can conclude from this that it is correctly learning the problem representation and a strategy capable of winning on a number of instances. Depending on the domains it is possible to create a common language for all, making the approach less domain dependent, although as we see in this work the language is focused on problems that are in two-dimensional grids. A requirement for the algorithm to work correctly is that the strategy can be described using the grammar, then in domains where the strategy is very complex it will be necessary to increase the complexity of the language. In this paper we propose a method for dealing with multi-agent domains, which manages to find policies that beat the adversary from any initial state where this is possible. This approach is a step in the explainability of reinforcement-produced policies, and it is also a step towards having transferable policies that capture the representation of the problem.


%\subsection{Discussion}






\newpage



\section{Apendix}

Here we show traces of the example domains, in which the learned policy is executed on small instances, against other agents and on larger instances.

Recall that the objects that can be found in the layouts are equivalent to:
\begin{Verbatim}[fontsize=\footnotesize]
  A: Agent
  D: destination
  T: adversary
  K: white-king
  Q: white-queen
  R: white-rook
  k: black-king
  m: martian
  p: pet
  t: token
\end{Verbatim}

\subsection{Space Invaders}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # m  m  .  m #
  # m  m  .  m #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  A  . #
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
    # m.mm.m # m.mm.m # m.mm.m # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # m.mm.m # m.mm.m # m.mm.. # m.mm.m # m.mm.. # m.mm.. # .m.mm. # .m.mm. # .m.mm. # ..m.mm #
    # m.mm.m # m.mm.. # m.mm.. # m.mm.. # m.mm.. # m.mm.. # .m.mm. # .m.mm. # .m.m.. # ..m.m. #
    # ...... # ...... # ...... # m.mm.. # m.mm.. # m.mm.. # .m.mm. # .m.m.. # .m.m.. # ..m.m. #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # .....A # .....A # .....A # .....A # .....A # ....A. # ....A. # ....A. # ....A. # ....A. # ...


    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ..m.mm # ..m.mm # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ..m.m. # ..m... # ..m.mm # ..m..m # ..m..m # ...... # ...... # ...... # ...... # ...... #
    # ..m... # ..m... # ..m... # ..m... # ..m... # ..m..m # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ..m... # ..m... # ..m... # ..m... # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ...... # ...... # ...... # ..m... # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ...... # .ee..... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
... # ....A. # ....A. # ....A. # ....A. # .....A # .....A # .....A # ....A. # ....A. # ...A.. # ...


    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...m.. # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...m.. # ...m.. # ...m.. # ...... #
    # ...... # ...m.. # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
... # ...A.. # ...A.. # ...A.. # ...A.. #
\end{Verbatim}


\subsection{Nim}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  . #
  # t  . #
  # t  . #
  # t  t #
  # t  t #
  # t  t #
\end{Verbatim}


\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
\end{Verbatim}

\subsection{Delivery}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  .  .  .  .  A #
  # .  D  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  p  .  .  . #
  # .  .  .  .  .  . #
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  p.... # p.... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... #
  ..... # .A... # .A... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... #
  A.... # ..... # ..... # ..A.. # .pA.. # ..A.. # ..... # ..... # ..... # ..... # ..... # ..... #
  ..... # ..... # ..... # ..... # ..... # ..... # ...A. # ..pA. # ...A. # ..... # ..... # ..... #
  ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ...pA # ....A # ....A #
\end{Verbatim}

\subsection{Shoot}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # . . . . #
  # A . . . #
  # . . T . #
  # . . . . #
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  # A....... # .A...... # ..A..... # ..A..... # ...A.... # ....A... # ....A... # .....A.. # ......A. #
  # ........ # ........ # ........ # ........ # ........ # ........ # .....T.. # ......T. # ......T. #
  # ........ # ........ # ........ # ........ # ........ # ........ # ........ # ........ # ........ #
  # .....T.. # .....T.. # .....T.. # ........ # ........ # ........ # ........ # ........ # ........ #
  # ........ # ........ # ........ # .....T.. # .....T.. # .....T.. # ........ # ........ # ........ # ...

     # ......A. # .....A.. # .....A.. #
     # ........ # ........ # ........ #
     # .....T.. # .....T.. # ........ #
     # ........ # ........ # ........ #
  ...# ........ # ........ # ........ #

\end{Verbatim}


\subsection{Checkmate in one with Queen}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  Q #
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  K  .  .  #  .  .  .  .  .  .  .  K  .  .  #
# .  .  .  .  .  .  .  .  .  k  #  .  .  .  .  .  .  .  .  Q  k  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  Q  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\subsection{Checkmate in one with Rook}

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  R #
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  K  .  k  #  .  .  .  .  .  .  .  K  .  k  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  R  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  R  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\newpage

%\paragraph{Bullet points}
%\begin{itemize}
%\item Learning generalized policies in unsupervised FOND domains from small examples.
%\item FOND encoding, introducing Good(s, a) features to deal with non-determinism. Where $a \in A$, and $A$ is not required to be fixed. However, if $A$ is fixed we show a way to exploit it reducing the number of transitions to distinguish.
%\item Being complete in domains where not all states have a good transition, with the optional constraint Bad(s).
%\item Empirical results with six different domains: Chase, Shoot, Space Invaders, Delivery, Checkmate with Queen, Checkmate with Rook.
%\item Incremental policy learning, to address the scalability problem. First a random sample is taken from the whole graph and the policy is then refined incrementally by iteratively increasing the sample size. The strategy for increasing the size of the sample can be goal-directed or random.
%%\item Introducing the sort of novelty to choose between the different successors classified as good by our policy
%\item Use of a uniform grammar for the different grid domains.
%\end{itemize}


\bibliographystyle{plain}
% Cross-referenced entries need to go after the entries that cross-reference them
\bibliography{abbrv-short,literatur,references,crossref-short}
\end{document}
