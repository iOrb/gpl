\documentclass[a4paper]{article}
%\def\year{2020}\relax
%\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
%\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
%\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage[margin=1in]{geometry}

% our packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{booktabs}
%\usepackage[inline]{enumitem}
%\usepackage{mathtools}
%\usepackage{multirow}
%\usepackage[mode=buildnew,subpreambles=true]{standalone}
\usepackage{subcaption}
%\usepackage{todonotes}
\usepackage[table]{xcolor}  % TODO Comment out for final submission
%\usepackage{pgfplots}
%\usepackage{tikz}
\usepackage{natbib}
\usepackage{fancyvrb}
\usepackage{color}



% Some useful macros
%\newcommand{\inlinecite}[1]{\citeauthor{#1}~(\citeyear{#1})}
%\newcommand{\citealp}[1]{\citeauthor{#1}~\citeyear{#1}}

\newcommand{\smallpar}[1]{{\vspace{10pt}\noindent \bf #1.}}

\newcommand{\free}[1]{\ensuremath{\mathrm{free}(#1)}}
\newcommand{\vars}{\ensuremath{\mathrm{vars}}}
\newcommand{\pre}{\ensuremath{\mathrm{pre}}}
\newcommand{\add}{\ensuremath{\mathrm{add}}}
\newcommand{\del}{\ensuremath{\mathrm{del}}}
\newcommand{\effs}{\ensuremath{\mathrm{effs}}}

\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\tuple}[1]{\tup{#1}}  % Just an alias
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\newcontent}{\textbf{\color{red}NEW}}

\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\newcommand{\wip}[1]{{\color{red} #1}}  % From "work in progress" :-)
\newcommand{\gfm}[1]{\footnote{\color{red}{[Guillem] #1}}}

\newcommand{\numtasks}[1]{\small{(#1)}}

\newcommand{\badtx}{\ensuremath{\mathrm{BAD}}}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title ()
/Author ()
} %Leave this

\title{Self-Supervised Learning of Generalized Policies in adversarial domains}
%Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\author{GFM}

\begin{document}

\maketitle


\subsection{SAT Encoding}

Our encoding is parametrized by
\begin{itemize}
 \item Pool $F$ of description logic features $f$, each with given feature complexity $\mathcal{K}(f)$.
 \item \newcontent{} A (training) set $T$ of transitions $(s, a, s') \in T$ from a number of instances of the same domain.
       We assume $T$ is \emph{closed}: if $s$ appears in the first position of a transition, then all possible
       transitions starting in $s$ in that instance appear in $T$ too.
 \item \newcontent{} A set $adv(s)$ of all possible moves by the adversary on state $s$.

 \item For each state $s$ appearing in some transition, full information on whether $s$
       is a goal, unsolvable, or alive.
       Also access to the minimum distance-to-goal $V^*(s)$ for each state $s$.
 \item A parameter $\delta$ which is a ``slack'' value to determine the maximum deviation from the optimal $V^*(s)$
 what we will allow in our policy. This will be made clearer in the encoding below.

 \item \newcontent{} A Boolean parameter \texttt{allow\_bad\_states} with intended meaning ``allow the policy
        to remain undefined for some of the states in the training set''.
 \item \newcontent{} A parameter \texttt{action\_labels} with possible values:
 \begin{itemize}
   \item \texttt{ignore}: Don't exploit the info on what action induced what transition. This is necessary
  when the set of actions changes over different instances of the domain.
   \item \texttt{use$_1$}: Use info on action labels, reasoning only about whether taking an action in a certain
   (abstract) state is good or not. This assumes that the action set is fixed over the entire domain.
   \item \texttt{use$_2$}: As above, but reason too about the feature change. E.g. your policy can have rules as
  ``When $f>0$, take action $a$ \emph{only if $f'$ increases}.
 \end{itemize}
\end{itemize}


\subsubsection{Main Variables}

\begin{itemize}
 \item $Good(s, a, s')$, for $(s, a, s') \in T$ with $s$ being alive.

 \item $Bad(s)$ for $s$ an alive state appearing in $T$.

 %\item $Good(s, s')$ for $s$ alive, $s'$ solvable and $(s, s') \not\in \badtx$.

 \item $V(s, d)$ for $s$ alive, and $d \in [0, D]$, where $D = \max_{s} \delta \cdot V^*(s)$,
 with intended denotation $V(s)=d$.
 Note that for states $s$ that are a goal, we know $V(s)=0$,
 and for states $s$ that are unsolvable, we know that $V(s) \neq d$ for all $d$.
 Thus, we can restrict SAT variables $V(s, d)$ to those states $s$ that are alive.

 \item $Select(f)$, for each feature $f$ in the feature pool.
\end{itemize}



\subsubsection{Hard Constraints}

\smallpar{C1 (if $\neg$\texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
\bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}

\smallpar{C1 (if \texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
Bad(t) \lor \bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}

\smallpar{C2} $V$ is always descending along Good actions, \textbf{taking into account all possible adversary responses}.
For each $(s, a, s')$ in the set of non-deterministic transitions
such that both $s$ and $s'$ are alive, post:
%Good(s, a) \land V(s, k) \rightarrow \bigvee_{1 \leq k' \leq k} V(s', k'),&\;\; \text{for } k \in [1, D].
\begin{align*}
Good(s, a, s') \rightarrow V(s'') < V(s)  \;\; \text{for } (s, a, s') \in T,  s'' \in adv(s').
\end{align*}

%\smallpar{C3} All descending transitions must be considered Good:
%\begin{align*}
% V(s, d) \land V(s'', d') \rightarrow Good(s, a),&\;\; \text{for $s$ alive, $(s, a, s'') \in T$, $1 \leq d' < d \leq D$,} \\
% Good(s, a),&\;\; \text{for $s$ alive, $s'$ goal.} \tag{\theequation${}^\prime$}
%\end{align*}

%\smallpar{C4-5} Any upper bound on $V(s)$ (for $s$ not a goal) needs to be justified:
%\begin{align*}
% V(s) \leq d+1 \rightarrow \bigvee_{\substack{
% s' \text{ goal child of } s\\
% (s, s') \not\in \badtx}} Good(s, s') \lor
% \bigvee_{\substack{
% s' \text{ alive child of } s\\
% (s, s') \not\in \badtx}} GV(s, s', d),&
% \;\; \text{for $s$ alive, $d \in [0, D)$.} \\
% \neg V(s) \leq 0,&\;\; \text{for $s$ not a goal.}
%\end{align*}


\smallpar{C3}
Variables $V(s, d)$ define a function that is total over the set of alive states,
and such that $V(s)$ is within lower bound $V^*(s)$ and upper bound $\delta \cdot V^*(s)$:
\begin{align*}
 \bigvee_{V^*(s) \leq d \leq \delta \cdot V^*(s)} V(s,d),&\;\; \text{for $s$ alive.} \\
 \neg V(s, d) \lor \neg V(s, d')&\;\; \text{for $s$ alive, $1 \leq d < d' \leq D$.}
\end{align*}


\smallpar{C4}
Good transitions can be distinguished from bad transitions.
%Let $(s, s')$ and $(t, t')$ be \emph{representative} transitions
%of two different equivalence classes such that $(s, s') \not\in \badtx$
%(which implies that $s'$ is solvable). Then,
Let $(s, a, s')$ and $(t, b, t')$ be \emph{representative} transitions
of two different equivalence classes such with $s$, $t$ alive. Then,
\begin{align*}
 Good(s, a, s') \land \neg Good(t, b, t') \rightarrow
 D2(s, s', t, t'),&\;\; \text{  }
% Good(s, a, s') \rightarrow
% D2(s, s', t, t'),&\;\; \text{for $s, t$ alive, $(t, t') \in \badtx$.}
\end{align*}

\noindent where $D2(s, s', t, t')$ is shorthand for $\bigvee_{} Select(f)$, with $f$ ranging over:
\begin{itemize}
 \item[(a)] all features that distinguish state $s$ from $t$, if \texttt{action\_labels = use$_1$}.
 \item[(b)] all features that distinguish $s$ from $t$ \textbf{plus} all features that
 change differently along $(s, s')$ than along $(t, t')$.
\end{itemize}

Additionally, if \texttt{action\_labels = use$_1$} or \texttt{action\_labels = use$_2$}, then
\textbf{the above constraint is only posted for transitions such that $a=b$}.


\smallpar{C5 (Optional)}
Goals are distinguishable from non-goals.
\begin{align*}
\bigvee_{f \in D1(s, s')} Select(f),&\;\; \text{for $s$ goal, $s$ not a goal}
\end{align*}


% GFM: I'm commenting this out, I don't think this is needed at all, actually I don't even know where this is coming from,
% maybe some old draft???
%\smallpar{C10 (Optional)}
%All selected features need to have some Good transition that takes them to $0$:
%\begin{align*}
% Selected(f) \rightarrow \bigvee_{(s, a, s') \in Z(f)} Good(s, s'),&\;\; \text{for $f$ in pool}
%\end{align*}
%
%\noindent where $Z(f)$ is the set transitions starting in an alive state that change the denotation of $f$ from something
%larger than 0 to 0.

\subsubsection{Soft Constraints}
\smallpar{C6}
A constraint $\neg Select(f)$ for each feature $f$ in the pool, with weight equal to its complexity $\mathcal{K}(f)$.

\smallpar{C7 (if \texttt{allow\_bad\_states})}
A constraint $\neg Bad(s)$ for each alive state $s$, with weight equal to one.

\pagebreak



\section{Terminology}
A state is called \emph{reachable} if there is a path to it from $s_0$, and
it is called \emph{solvable} if there is a path from it to a goal
state.
A state is \emph{alive} if it is solvable, reachable and not a
goal state~\cite{frances-et-al-ijcai2019}.
We use $T$ to denote the set of all transitions $(s, s')$ in the training sample such that $s$ is alive.

\subsection{Optimizations}

\paragraph{Non-distinguishability of transitions as an equivalence relation.}
Any fixed, given pool of features $F$ implicitly defines an equivalence relation where to transitions are
equivalent iff they cannot be distinguished \emph{by any feature in $F$}.
If two transitions cannot be distinguished by any feature, then clearly either the policy computed by the SAT solver
considers all of them as ``good'', or as ``bad''.
We'll exploit this by using one single SAT variable to denote whether \emph{all transitions in a given equivalence
class} are good or bad. When exploiting this notion of equivalence (which is implemented as an optional feature of
the CNF generator), then every mention below to SAT variable $Good(s, s')$ needs to be read as $Good(s_{\star}, s_{\star}')$,
where $(s_{\star}, s_{\star}')$ is the \emph{representative} transition of the equivalence class to which $(s, s')$ belongs.

\paragraph{``Bad'' transitions.}
We use \badtx{} to denote the set of transitions that have been determined at preprocessing as necessarily
\emph{not} good.
At the moment, this set contains all transitions that go from an alive to an unsolvable state and, if using
equivalence relations, all those other transitions in whose equivalence class there is some other ``bad'' transition.


\newpage

\section{Empirical Results}


\subsection{Space Invaders - v0}
\subsubsection{Description}
In this domain the agent can shoot, move to the right or to the left. The agent $A$ has two actions per turn and the mastians $m$ one. To win the agent has to kill all the martians, for this he must shoot each of them. The agent kills an opponent if he shoots while being in the same column, and he can only kill one for each shoot. The martians has three possible actions, left, right and down, all the adversaries move in the same direction. The agent cannot be destroyed by the opponents. The agent can select a Martian as a target $M$.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  #m...m#
  #m...M#
  #.....#
  #...A.#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
(A) Num[cell-has-martian] [k=1]
(B) Bool[cell-has-target_martian] [k=1]
(C) Dist[col-has-agent;adjacent_col;col-has-target_martian] [k=4]:
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of martians\\
$B$. There is some target martian\\
$C$. Absolute horizontal distance between agent and target martian

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
    1. B>0 AND C>0 -> {A ·, B ·, C ↓}
    2. A=0 AND B>0 AND C=0 -> {A ·, B ↓, C ↑}
    3. A>0 AND B>0 AND C=0 -> {A ↓, B ·}
\end{Verbatim}

\subsubsection{Policy Description}
According to the first rule if the distance between the agent and the target martian is greater than zero, it is good to decrease this distance, i.e. move towards the target martian. The third rule is applicable when there are several Martians and the agent is in the same column as the targeted Martian, and indicates that in this abstraction of the state it is good to decrease the number of Martians. The second rule indicates that if there is only one Martian left, and we are below him, it is good to eliminate him.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  m..m # m..m # m..m # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... #
  M..m # M..m # M..m # m..m # M..m # ...m # ..m. # ..m. # ..m. # ...m # ...m # ...M # ..M. # ..M. # .... #
  .... # .... # .... # M..m # ...m # ...M # ..M. # ..M. # ..M. # ...M # ...M # .... # .... # .... # .... #
  .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... #
  ..A. # .A.. # A... # A... # A... # A... # A... # .A.. # ..A. # ..A. # ...A # ...A # ...A # ..A. # ..A. #
\end{Verbatim}

\subsection{Space Invaders - v1}
\subsubsection{Description}
In this domain the agent $A$ can move to the right or to the left. The agent has two actions per turn and the martians $m$ have one. To win the agent has to kill all the martians, for this he must shoot each of them. The agent kills an opponent if he moves into the column of the adversary, and he can only kill one for each shoot. The adversary has three possible actions, left, right and down, and all the adversaries move in the same direction. The agent cannot be destroyed by the opponents. The agent can select a Martian as a target $M$.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  #m...m#
  #m...M#
  #.....#
  #...A.#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Num[cell-has-martian] [k=1]:
  (B) Bool[cell-has-target_martian] [k=1]:
  (C) Dist[col-has-agent;adjacent_col;col-has-target_martian] [k=4]:
  absolute horizontal distance between agent and target martian
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of martians\\
$B$. There is some target martian\\
$C$. Absolute horizontal distance between agent and target martian

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. B>0 AND C=0 -> {A ·, B ·, C ↑}
  2. B>0 AND C>0 -> {A ·, B ·, C ↓}
  3. A=0 AND B>0 AND C>0 -> {A ·, B ↓}
  4. A>0 AND B>0 AND C>0 -> {A ↓, B ·}
\end{Verbatim}

\subsubsection{Policy Description}
In the $Space Invaders - v1$ the agent cannot shoot, but he can kill the Martians by positioning himself in their column. According to the second rule, if the agent is not in the column of the target Martian, it is good to move closer to this column. According to the first rule of the policy, if we are below the target Martian, it is good to increase the horizontal distance between him and the agent. Intuitively, if the agent is under the target Martian, it is good to for the agent to moves away from the target martinan and then get in his column and finish him off. Finally, rules three and four say that transitions to states where the number of Martians has decreased are good.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  m..m # m..M # m..M # .... # .... # .... # .... # .... # .... # .... # .... # .... #
  m..M # m... # m... # m..M # m... # m... # .m.. # .M.. # .M.. # M... # M... # .... #
  .... # .... # .... # m... # M... # M... # .M.. # .... # .... # .... # .... # .... #
  .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... # .... #
  ..A. # ...A # ..A. # ..A. # ...A # ..A. # ..A. # .A.. # ..A. # ..A. # .A.. # A... #
  1 (p)  1 (p)  2 (p)  1 (p)  1 (p)  2 (p)  1 (p)  1 (p)  2 (p)  1 (p)  1 (p)  2 (p)
\end{Verbatim}

\subsection{Chase - v0}
\subsubsection{Description}
In this domain an agent $A$ has to chase an adversary $T$. The goal is defined by the agent being in a adversary's adjacent cell. The set of possile actions for the agent is ~\{$rightup$, $rightdown$, $leftdown$, $leftup\}$.
The set of possile action for the target is ~\{$up$, $right$, $down$, $left$, $leftup\}$. The agent can move only diagonally and the adversary can move in perpendicular directions.  The agent and the adversary can move only once per turn.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
#A.....#
#......#
#......#
#.....T#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-hv-AGENT,Exists(adjacent_col,col-hv-TARGET))] [k=5]:
  the column of the agent is adjacent to the column of the adversary

  (B) Bool[And(row-hv-AGENT,Exists(adjacent_row,row-hv-TARGET))] [k=5]:
  the row of the agent is adjacent to the row of the adversary

  (C) Dist[row-hv-AGENT;adjacent_row;row-hv-TARGET] [k=4]:
  absolute vertical distance between agent and adversary

  (D) Dist[col-hv-TARGET;adjacent_col;col-hv-AGENT] [k=4]:
  absolute horizontal distance between agent and adversary
\end{Verbatim}

\subsubsection{Policy (with no fixed action space)}

\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B=0 AND D>0 AND C=0 -> {A ↑, B ↑, C ↑, D ↓}, {A ·, B ↑, C ↑, D ↓}
  2. A=0 AND B=0 AND D>0 AND C>0 -> {A ·, B ↑, C ↓, D ↓}, {A ↑, B ↑, C ↓, D ↓},
                                    {A ·, B ·, C ↓, D ↓}, {A ↑, B ·, C ↓, D ↓}
  3. A=0 AND B>0 AND D>0 AND C>0 -> {A ·, B ↓, C ↓, D ↓}, {A ↑, B ↓, C ↓, D ↓}
  4. A>0 AND B=0 AND D>0 AND C>0 -> {A ↓, B ·, C ↓, D ↓}, {A ↓, B ↑, C ↓, D ↓}
  5. A=0 AND B=0 AND D=0 AND C>0 -> {A ↑, B ·, C ↓, D ↑}, {A ↑, B ↑, C ↓, D ↑}


\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  A..... # ...... # ...... # ...... # ...... # ...... # .....T # ....AT #
  ...... # .A.... # .A.... # ...... # .....T # ...A.T # ...A.. # ...... #
  ...... # ...... # .....T # ..A..T # ..A... # ...... # ...... # ...... #
  .....T # .....T # ...... # ...... # ...... # ...... # ...... # ...... #
\end{Verbatim}


\subsection{Chase - v1}
\subsubsection{Description}
In this domain an agent has to chase an adversary. The goal is defined by the agent being in a target adjacent cell. The set of possile actions for the agent and the adversary is ~\{$rightup$, $rightdown$, $leftdown$, $leftup\}$. The agent can move twice per turn and the adversary can move once per turn.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
#A.....#
#......#
#......#
#.....T#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-hv-AGENT,Exists(adjacent_col,col-hv-TARGET))] [k=5]:
  the column of the agent is adjacent to the column of the adversary

  (B) Bool[And(row-hv-AGENT,Exists(adjacent_row,row-hv-TARGET))] [k=5]:
  the row of the agent is adjacent to the row of the adversary

  (C) Dist[row-hv-AGENT;adjacent_row;row-hv-TARGET] [k=4]:
  absolute vertical distance between agent and adversary

  (D) Dist[col-hv-TARGET;adjacent_col;col-hv-AGENT] [k=4]:
  absolute horizontal distance between agent and adversary
\end{Verbatim}

\subsubsection{Policy (with no fixed action space)}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B=0 AND C>0 -> {A ·, B ↑, C ↓, D ·}, {A ·, B ·, C ↓, D ·}
  2. A=0 AND B>0 AND D>0 AND C>0 -> {A ·, B ·, C ·, D ↓}
  3. A=0 AND B=0 AND D>0 AND C=0 -> {A ·, B ·, C ·, D ↓}, {A ↑, B ·, C ·, D ↓}
  4. B=0 AND D>0 AND C>0 -> {A ·, B ↑, C ↓, D ·}, {A ·, B ·, C ↓, D ·}
  5. A=0 AND D>0 AND C>0 -> {A ↑, B ·, C ·, D ↓}
  6. A>0 AND B=0 AND D>0 AND C>0 -> {A ·, B ↑, C ↓, D ·}, {A ·, B ·, C ↓, D ·}
  7. A=0 AND B=0 AND D=0 AND C>0 -> {A ·, B ·, C ↓, D ·}, {A ·, B ↑, C ↓, D ·}

\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  ...... # ...... # ...... # .T.... # .T.... # .T.... # T..... # T..... # T..... #
  .T.... # .T.... # .T.... # ...... # ....A. # ...A.. # ...A.. # ..A... # .A.... #
  ...... # .....A # ....A. # ....A. # ...... # ...... # ...... # ...... # ...... #
  .....A # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
\end{Verbatim}

\subsection{Nim - v0}
\subsubsection{Description}
In this game the action space is equal to the number of tokens $t$ in the grid. In order to win the agent has to remove the last token in his turn. If the opponet removes the last token in his turn, the agent loses. The agent and the adversary has on action per turn. When one token is removed, the tokens above in the same column are removed too.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  # t  . #
  # t  . #
  # t  t #
  # t  t #
  # t  t #
  # t  t #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Num[And(cell-has-token,Exists(left_cell,cell-has-token))] [k=5]:
  number of tokens with a token immediately to the left of it

  (B) Num[And(cell-has-token,Forall(leftdown_cell,cell-has-token))] [k=5]:
  number of tokens with a token immediately to the leftdown of it

  (C) Num[And(cell-has-token,Forall(rightdown_cell,cell-has-token))] [k=5]:
  number of tokens with a token immediately to the rightdown of it

\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B>0 AND C>0 -> {A ·, B ↓, C ↓}
  2. B>0 AND C>0 -> {A ·, B ↓, C ↓}

\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #
  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
\end{Verbatim}


\subsection{Delivery - v0}
\subsubsection{Description}

In this domain the agent has to pick up a missed pet $p$ and take it to a certain location, the destination $D$. The pet has the option on its turn to escape from its captor to any of the cells adjacent to it. The goal is defined by the agent in a cell adjacent to the destination while carrying the pet. The agent has two actions per turn and the adversary one. The set of possile actions for the agent is ~\{$rightup$, $rightdown$, $leftdown$, $leftup\}$.  The set of possile actions for the pet consist in escape to one of the adjacent cells to the agent, i.e. at most eight possible actions depending on the position of the agent in the grid. The pet can not escape to the delivey point.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  # D  .  .  .  .  A #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  p #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Atom[holding_pet] [k=1]:
  the agent is holding the pet

  (B) Bool[And(cell-has-agent,Exists(adjacent_cell,cell-has-destination))] [k=5]:
  the destination is adjacent to the agent

  (C) Dist[cell-has-agent;adjacent_cell;cell-has-destination] [k=4]:
  manhattan distance between agent and destination

  (D) Dist[cell-has-agent;adjacent_cell;cell-has-pet] [k=4]:
  manhattan distance between agent and pet
\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. B=0 AND C>0 AND D>0 -> {A ·, B ·, C ↓, D ·}
  2. A=0 AND B=0 AND C>0 AND D>0 -> {A ·, B ·, C ↓, D ↓}, {A ·, B ·, C ↑, D ↓},
                                    {A ↑, B ·, C ·, D ↑}, {A ·, B ·, C ·, D ↓}
  3. A>0 AND B=0 AND C>0 AND D>0 -> {A ·, B ↑, C ↓, D ·}
  4. A=0 AND B>0 AND C>0 AND D>0 -> {A ·, B ↓, C ↑, D ↓}

\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  p.... # p.... # p.... # p.... # ..... # ..... # ..... # ..... # ..... # ..... # ..... #
  ..... # ..... # ..... # .A... # .A... # pA... # .A... # ..... # ..... # ..... # ..... #
  ..... # ..... # A.... # ..... # ..... # ..... # ..... # ..A.. # .pA.. # ..A.. # ..... #
  ..... # .A... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ...A. #
  A...D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D #
\end{Verbatim}

\subsection{Shoot - v0}
\subsubsection{Description}
In this domain an agent has to chase an adversary. The goal is defined by the agent being in the column or row of the adversary. The set of possile actions for the agent and the target is ~\{$rightup$, $rightdown$, $leftdown$, $leftup\}$.  The agent and the target can move once per turn.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
#A.....#
#......#
#......#
#.....T#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-AGENT,col-has-TARGET)] [k=3]
  (B) Bool[And(col-has-AGENT,Exists(adjacent_col,col-has-TARGET))] [k=5]
  (C) Dist[row-has-TARGET;adjacent_row;row-has-AGENT] [k=4]

\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B=0 AND C>0 -> {A ·, B ·, C ↓}
  2. A=0 AND B>0 AND C>0 -> {A ↑, B ↓, C ·}
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  A...... # ....... # ....... # ....... # ....... # ....... #
  ....... # A...... # ....... # ....... # ....... # ....... #
  ....... # ....... # A...... # ....... # ....... # ....... #
  ....... # ....... # ....... # A...... # ....... # ....... #
  ....... # .....T. # ....... # ....... # A...... # ....... #
  .....T. # ....... # .....T. # ....... # ....... # A.....T #
  ....... # ....... # ....... # .....T. # ......T # ....... #
\end{Verbatim}

\subsection{Shoot - v1}
\subsubsection{Description}
In this domain an agent has to chase an adversary. The goal is defined by the agent being in the column or row of the adversary. The set of possile actions for the agent is ~\{$rightup$, $rightdown$, $leftdown$, $leftup\}$.
The set of possile action for the target is ~\{$up$, $right$, $down$, $left$, $leftup\}$.  The agent and the target can move once per turn.

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-AGENT,col-has-TARGET)] [k=3]
  (B) Bool[And(row-has-AGENT,row-has-TARGET)] [k=3]
  (C) Dist[col-has-TARGET;adjacent_col;col-has-AGENT] [k=4]
  (D) Dist[row-has-TARGET;adjacent_row;row-has-AGENT] [k=4]

\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B=0 AND C>0 AND D>0 -> {A ·, B ↑, C ↑, D ↓}, {A ↑, B ·, C ↓, D ↑},
                                    {A ↑, B ·, C ↓, D ↓}, {A ·, B ·, C ↓, D ↓},
                                    {A ·, B ↑, C ↓, D ↓}

\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  A...... # ....... # ....... # ....... # ....... # ....... #
  ....... # A...... # ....... # ....... # ....... # ....... #
  ....... # ....... # A...... # ....... # ....... # ....... #
  ....... # ....... # ....... # A...... # ....... # ....... #
  ....... # .....T. # ....... # ....... # A...... # ....... #
  .....T. # ....... # .....T. # ....... # ....... # A.....T #
  ....... # ....... # ....... # .....T. # ......T # ....... #
\end{Verbatim}

\subsection{Checkmate - v0}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the grid. In order to win the agent has to do a checkmate in one movement. Otherwise he loses. In $Checkmate-v0$ white pieces have the king $K$ and the queen $Q$, and black pieces have the king $k$.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  Q #
\end{Verbatim}


\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_queen)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_king)] [k=3]
  (D) Bool[And(row-has-black_king,row-has-white_queen)] [k=3]
  (E) Bool[And(cell-has-black_king,Exists(adjacent_cell,cell-has-white_queen))] [k=5]
  (F) Bool[And(col-has-white_king,Exists(adjacent_col,col-has-white_queen))] [k=5]
  (G) Bool[And(row-has-black_king,Exists(adjacent_row,row-has-white_king))] [k=5]
\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ·, F ·, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ↑, F ·, G ·}
  2. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ·, F ↓, G ·}
  3. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ·, F ·, G ·}
  4. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ↑, F ↑, G ·},
                                                            {A ·, B ·, C ·, D ·, E ↑, F ↑, G ·}
  5. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ↑, F ↓, G ·},
                                                            {A ·, B ·, C ·, D ↑, E ·, F ↓, G ·}
  6. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ↓, G ·}
  7. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ↑, F ·, G ·},
                                                            {A ·, B ·, C ·, D ·, E ↑, F ·, G ·}
  8. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ·, G ·}
  9. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·}
  10. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ·, G ·}
  11. A>0 AND B>0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↓, C ·, D ↑, E ·, F ·, G ·}
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  K  .  .  #  .  .  .  .  .  .  .  K  .  .  #
  .  .  .  .  .  .  .  .  .  k  #  .  .  .  .  .  .  .  .  Q  k  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  Q  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\subsection{Checkmate - v1}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the grid. In order to win the agent has to do a checkmate in one movement. Otherwise he loses. In $Checkmate-v1$ white pieces have the king and the rook $R$, and black pieces have the king.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  R #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_rook)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_rook)] [k=3]

\end{Verbatim}

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 -> {A ·, B ·, C ↑}
  2. A=0 AND B=0 AND C=0 -> {A ·, B ↑, C ·}
\end{Verbatim}

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  K  .  k  #  .  .  .  .  .  .  .  K  .  k  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  .  .  R  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  R  #
  .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\newpage

\paragraph{Bullet points}
\begin{itemize}
\item Learning generalized policies in unsupervised FOND domains from small examples.
\item FOND encoding, introducing Good(s, a) features to deal with non-determinism. Where $a \in A$, and $A$ is not required to be fixed. However, if $A$ is fixed we show a way to exploit it reducing the number of transitions to distinguish.
\item Being complete in domains where not all states have a good transition, with the optional constraint Bad(s).
\item Empirical results with five domains: Chase, Shoot, Space Invaders, Delivery, Wumpus world.
\item Incremental policy learning, to address the scalability problem. First a random sample is taken from the whole graph and the policy is then refined incrementally by iteratively increasing the sample size. The strategy for increasing the size of the sample can be goal-directed or random.
%\item Introducing the sort of novelty to choose between the different successors classified as good by our policy
\item Use of a uniform grammar for the different grid domains.
\end{itemize}


\bibliographystyle{plain}
% Cross-referenced entries need to go after the entries that cross-reference them
\bibliography{abbrv-short,literatur,references,crossref-short}
\end{document}
