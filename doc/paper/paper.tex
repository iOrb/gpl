\documentclass[a4paper]{article}
%\def\year{2020}\relax
%\usepackage{aaai20}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
%\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
%\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage[margin=1in]{geometry}

% our packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bm}
\usepackage{booktabs}
%\usepackage[inline]{enumitem}
%\usepackage{mathtools}
%\usepackage{multirow}
%\usepackage[mode=buildnew,subpreambles=true]{standalone}
\usepackage{subcaption}
%\usepackage{todonotes}
\usepackage[table]{xcolor}  % TODO Comment out for final submission
%\usepackage{pgfplots}
%\usepackage{tikz}
\usepackage{natbib}
\usepackage{fancyvrb}
\usepackage{color}



% Some useful macros
%\newcommand{\inlinecite}[1]{\citeauthor{#1}~(\citeyear{#1})}
%\newcommand{\citealp}[1]{\citeauthor{#1}~\citeyear{#1}}

\newcommand{\smallpar}[1]{{\vspace{10pt}\noindent \bf #1.}}

\newcommand{\free}[1]{\ensuremath{\mathrm{free}(#1)}}
\newcommand{\vars}{\ensuremath{\mathrm{vars}}}
\newcommand{\pre}{\ensuremath{\mathrm{pre}}}
\newcommand{\add}{\ensuremath{\mathrm{add}}}
\newcommand{\del}{\ensuremath{\mathrm{del}}}
\newcommand{\effs}{\ensuremath{\mathrm{effs}}}

\newcommand{\tup}[1]{\ensuremath{\langle #1 \rangle}}
\newcommand{\tuple}[1]{\tup{#1}}  % Just an alias
\newcommand{\set}[1]{\ensuremath{\left\{#1 \right\}}}
\newcommand{\setst}[2]{\ensuremath{\left\{#1 \mid #2 \right\}}}
\newcommand{\newcontent}{\textbf{\color{red}NEW}}

\newtheorem{definition}{Definition}
\newtheorem{definitionandtheorem}[definition]{Definition and Theorem}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{example}[definition]{Example}

\newcommand{\wip}[1]{{\color{red} #1}}  % From "work in progress" :-)
\newcommand{\gfm}[1]{\footnote{\color{red}{[Guillem] #1}}}

\newcommand{\numtasks}[1]{\small{(#1)}}

\newcommand{\badtx}{\ensuremath{\mathrm{BAD}}}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
 \pdfinfo{
/Title ()
/Author ()
} %Leave this

\title{Self-Supervised Learning of Generalized Policies in adversarial domains}
%Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
%\author{GFM}

\begin{document}

\maketitle


\subsection{SAT Encoding}

Our encoding is parametrized by
\begin{itemize}
 \item Pool $F$ of description logic features $f$, each with given feature complexity $\mathcal{K}(f)$.
 \item \newcontent{} A (training) set $T$ of transitions $(s, a, s') \in T$ from a number of instances of the same domain.
       We assume $T$ is \emph{closed}: if $s$ appears in the first position of a transition, then all possible
       transitions starting in $s$ in that instance appear in $T$ too.
 \item \newcontent{} A set $adv(s)$ of all possible moves by the adversary on state $s$.

 \item For each state $s$ appearing in some transition, full information on whether $s$
       is a goal, unsolvable, or alive.
       Also access to the minimum distance-to-goal $V^*(s)$ for each state $s$.
 \item A parameter $\delta$ which is a ``slack'' value to determine the maximum deviation from the optimal $V^*(s)$
 what we will allow in our policy. This will be made clearer in the encoding below.

 \item \newcontent{} A Boolean parameter \texttt{allow\_bad\_states} with intended meaning ``allow the policy
        to remain undefined for some of the states in the training set''.
 \item \newcontent{} A parameter \texttt{action\_labels} with possible values:
 \begin{itemize}
   \item \texttt{ignore}: Don't exploit the info on what action induced what transition. This is necessary
  when the set of actions changes over different instances of the domain.
   \item \texttt{use$_1$}: Use info on action labels, reasoning only about whether taking an action in a certain
   (abstract) state is good or not. This assumes that the action set is fixed over the entire domain.
   \item \texttt{use$_2$}: As above, but reason too about the feature change. E.g. your policy can have rules as
  ``When $f>0$, take action $a$ \emph{only if $f'$ increases}.
 \end{itemize}
\end{itemize}


\subsubsection{Main Variables}

\begin{itemize}
 \item $Good(s, a, s')$, for $(s, a, s') \in T$ with $s$ being alive.

 \item $Bad(s)$ for $s$ an alive state appearing in $T$.

 %\item $Good(s, s')$ for $s$ alive, $s'$ solvable and $(s, s') \not\in \badtx$.

 \item $V(s, d)$ for $s$ alive, and $d \in [0, D]$, where $D = \max_{s} \delta \cdot V^*(s)$,
 with intended denotation $V(s)=d$.
 Note that for states $s$ that are a goal, we know $V(s)=0$,
 and for states $s$ that are unsolvable, we know that $V(s) \neq d$ for all $d$.
 Thus, we can restrict SAT variables $V(s, d)$ to those states $s$ that are alive.

 \item $Select(f)$, for each feature $f$ in the feature pool.
\end{itemize}


\subsubsection{Hard Constraints}

\smallpar{C1 (if $\neg$\texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
\bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}

\smallpar{C1 (if \texttt{allow\_bad\_states})}
Policy is defined in all alive states:
\begin{align*}
Bad(t) \lor \bigvee_{(s, a, s') \in T \text{ s.t. } s=t} Good(s, a, s'), \;\; \text{for $t$ alive in $T$.}
\end{align*}

\smallpar{C2} $V$ is always descending along Good actions, \textbf{taking into account all possible adversary responses}.
For each $(s, a, s')$ in the set of non-deterministic transitions
such that both $s$ and $s'$ are alive, post:
%Good(s, a) \land V(s, k) \rightarrow \bigvee_{1 \leq k' \leq k} V(s', k'),&\;\; \text{for } k \in [1, D].
\begin{align*}
Good(s, a, s') \rightarrow V(s'') < V(s)  \;\; \text{for } (s, a, s') \in T,  s'' \in adv(s').
\end{align*}

%\smallpar{C3} All descending transitions must be considered Good:
%\begin{align*}
% V(s, d) \land V(s'', d') \rightarrow Good(s, a),&\;\; \text{for $s$ alive, $(s, a, s'') \in T$, $1 \leq d' < d \leq D$,} \\
% Good(s, a),&\;\; \text{for $s$ alive, $s'$ goal.} \tag{\theequation${}^\prime$}
%\end{align*}

%\smallpar{C4-5} Any upper bound on $V(s)$ (for $s$ not a goal) needs to be justified:
%\begin{align*}
% V(s) \leq d+1 \rightarrow \bigvee_{\substack{
% s' \text{ goal child of } s\\
% (s, s') \not\in \badtx}} Good(s, s') \lor
% \bigvee_{\substack{
% s' \text{ alive child of } s\\
% (s, s') \not\in \badtx}} GV(s, s', d),&
% \;\; \text{for $s$ alive, $d \in [0, D)$.} \\
% \neg V(s) \leq 0,&\;\; \text{for $s$ not a goal.}
%\end{align*}


\smallpar{C3}
Variables $V(s, d)$ define a function that is total over the set of alive states,
and such that $V(s)$ is within lower bound $V^*(s)$ and upper bound $\delta \cdot V^*(s)$:
\begin{align*}
 \bigvee_{V^*(s) \leq d \leq \delta \cdot V^*(s)} V(s,d),&\;\; \text{for $s$ alive.} \\
 \neg V(s, d) \lor \neg V(s, d')&\;\; \text{for $s$ alive, $1 \leq d < d' \leq D$.}
\end{align*}


\smallpar{C4}
Good transitions can be distinguished from bad transitions.
%Let $(s, s')$ and $(t, t')$ be \emph{representative} transitions
%of two different equivalence classes such that $(s, s') \not\in \badtx$
%(which implies that $s'$ is solvable). Then,
Let $(s, a, s')$ and $(t, b, t')$ be \emph{representative} transitions
of two different equivalence classes such with $s$, $t$ alive. Then,
\begin{align*}
 Good(s, a, s') \land \neg Good(t, b, t') \rightarrow
 D2(s, s', t, t'),&\;\; \text{  }
% Good(s, a, s') \rightarrow
% D2(s, s', t, t'),&\;\; \text{for $s, t$ alive, $(t, t') \in \badtx$.}
\end{align*}

\noindent where $D2(s, s', t, t')$ is shorthand for $\bigvee_{} Select(f)$, with $f$ ranging over:
\begin{itemize}
 \item[(a)] all features that distinguish state $s$ from $t$, if \texttt{action\_labels = use$_1$}.
 \item[(b)] all features that distinguish $s$ from $t$ \textbf{plus} all features that
 change differently along $(s, s')$ than along $(t, t')$.
\end{itemize}

Additionally, if \texttt{action\_labels = use$_1$} or \texttt{action\_labels = use$_2$}, then
\textbf{the above constraint is only posted for transitions such that $a=b$}.


\smallpar{C5 (Optional)}
Goals are distinguishable from non-goals.
\begin{align*}
\bigvee_{f \in D1(s, s')} Select(f),&\;\; \text{for $s$ goal, $s$ not a goal}
\end{align*}


% GFM: I'm commenting this out, I don't think this is needed at all, actually I don't even know where this is coming from,
% maybe some old draft???
%\smallpar{C10 (Optional)}
%All selected features need to have some Good transition that takes them to $0$:
%\begin{align*}
% Selected(f) \rightarrow \bigvee_{(s, a, s') \in Z(f)} Good(s, s'),&\;\; \text{for $f$ in pool}
%\end{align*}
%
%\noindent where $Z(f)$ is the set transitions starting in an alive state that change the denotation of $f$ from something
%larger than 0 to 0.

\subsubsection{Soft Constraints}
\smallpar{C6}
A constraint $\neg Select(f)$ for each feature $f$ in the pool, with weight equal to its complexity $\mathcal{K}(f)$.

\smallpar{C7 (if \texttt{allow\_bad\_states})}
A constraint $\neg Bad(s)$ for each alive state $s$, with weight equal to one.

\pagebreak



\section{Terminology}
A state is called \emph{reachable} if there is a path to it from $s_0$, and
it is called \emph{solvable} if there is a path from it to a goal
state.
A state is \emph{alive} if it is solvable, reachable and not a
goal state~\cite{frances-et-al-ijcai2019}.
We use $T$ to denote the set of all transitions $(s, s')$ in the training sample such that $s$ is alive.

\subsection{Optimizations}

\paragraph{Non-distinguishability of transitions as an equivalence relation.}
Any fixed, given pool of features $F$ implicitly defines an equivalence relation where to transitions are
equivalent iff they cannot be distinguished \emph{by any feature in $F$}.
If two transitions cannot be distinguished by any feature, then clearly either the policy computed by the SAT solver
considers all of them as ``good'', or as ``bad''.
We'll exploit this by using one single SAT variable to denote whether \emph{all transitions in a given equivalence
class} are good or bad. When exploiting this notion of equivalence (which is implemented as an optional feature of
the CNF generator), then every mention below to SAT variable $Good(s, s')$ needs to be read as $Good(s_{\star}, s_{\star}')$,
where $(s_{\star}, s_{\star}')$ is the \emph{representative} transition of the equivalence class to which $(s, s')$ belongs.

\paragraph{``Bad'' transitions.}
We use \badtx{} to denote the set of transitions that have been determined at preprocessing as necessarily
\emph{not} good.
At the moment, this set contains all transitions that go from an alive to an unsolvable state and, if using
equivalence relations, all those other transitions in whose equivalence class there is some other ``bad'' transition.

\newpage

\section{Grammar}

The sorts used are: $cell$, $row$, $column$ and . The constants for the sort $cell$ have the form $cell.r\_c$, this constant refers to the cell that is at the same time in the row $r$ and in the column $c$. The constants for sort $row$ are $row.r$, and this constant refers to the row $r$. The constants for sort $column$ are $column.c$, and this constant refers to column $c$. Each sort can be linked to an object, using a one-arity predicate, depending on the objects of the domain, given by a predicate of arity one with any variable belonging to the sorts $cell$, $row$ or $column$. An example for the sort $cell$ would be $cell\_has\_v(cell)$, where the variable of type $cell$ can take the value of $cell.r\_c$, and this predicate would indicate that the cell $cell.r\_c$ has an object with value $v$, which is the name of the object currently in $cell.r\_c$. To map the relative position between constants of the same sort, we use two-arity predicates, which have as variables these constants belonging to the same sort. For example, $adjacent\_cell(cell, cell)$ takes two constants of the sort $cell$, and means that the two cells are adjacent. The same philosophy would apply to the predicates $adjacent\_col(column, column)$ or $adjacent\_row(row, row)$.


\newpage

\section{Empirical Results}

In all domains we managed to solve with the shown policies any instance of the problem class, of any size. The policies have been found by learning unsupervised form from the tranining instances shown in the domains.

The sections broken down into each domain are: Description of the domain, the training instances used to discover the policy, the learned features, description of the features, learned policy, description of the policy, and finally an example of a possible game, from initial state to goal, using the discovered policy in a test instance against an expert adversary. The initial state of the example test instance is taken from the infinite set of possible instances in the domain.

Layouts have been created to show the problems and examples of game traces. The objects that can be found in the layouts are equivalent to:
\begin{Verbatim}[fontsize=\footnotesize]
  A: Agent
  D: destination
  T: adversary
  K: white-king
  Q: white-queen
  R: white-rook
  k: black-king
  m: martian
  p: pet
  t: token
\end{Verbatim}

\subsection{Space Invaders}
\subsubsection{Description}
This domain is a simplified version of the original Space Invaders in which the Martians cannot shoot and cannot move below the penultimate row. In this domain the agent can $shoot$, move to the $right$ or to the $left$. The agent during his turn can do two actions, while the Martians can do one. Note that if the Martians and the agent have only one action per turn the agent cannot win from all possible states in the game, this is because of the situation where the agent stands in the Martian's column to shoot him on his next turn, but the next turn (the Martian's turn) the Martian can move away, this leads to an infinite loop if both are playing perfectly. That is, the agent must be able to move to the agent's column and then shoot. To win the agent has to kill all the martians, for this he must shoot each of them. The agent kills an opponent if he shoots while being in the same column, and he can only kill one for each shoot. The martians has three possible actions $left$, $right$, and $down$. All the adversaries move in the same direction. The agent cannot be destroyed in any way by the Martians, they can go down to the row above the agent. The agent cannot collide with the Martians as they cannot descend to the row above it. Note that the policy found is optimal and will solve all possible instances of the game. The policy is also optimal for a version of the game where the Martians destroy the agent when they reach the row above him.

\subsubsection{Training instances:}
\begin{Verbatim}[fontsize=\footnotesize]
  # m  m  .  m #
  # m  m  .  m #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  .  . #
  # .  .  A  . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]

(A) Num[cell-has-martian] [k=1]
(B) Dist[col-has-agent;adjacent_col;col-has-martian] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of martians\\
$B$. Minimum absolute horizontal distance between a column with an agent and a column with a martian

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B>0 -> {A ·, B ↓}
  2. A>0 AND B=0 -> {A ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the first rule if the agent is not in the column of a Martian, $B$ is greater than zero, it is good to decrement $B$, i.e. move agent towards the closest column with martians. The second rule is applicable when the agent is under a Martian, in the same column, and according to this rule it is good to decrease the number of Martians, i.e. execute the action $shoot$.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
    # m.mm.m # m.mm.m # m.mm.m # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # m.mm.m # m.mm.m # m.mm.. # m.mm.m # m.mm.. # m.mm.. # .m.mm. # .m.mm. # .m.mm. # ..m.mm #
    # m.mm.m # m.mm.. # m.mm.. # m.mm.. # m.mm.. # m.mm.. # .m.mm. # .m.mm. # .m.m.. # ..m.m. #
    # ...... # ...... # ...... # m.mm.. # m.mm.. # m.mm.. # .m.mm. # .m.m.. # .m.m.. # ..m.m. #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # .....A # .....A # .....A # .....A # .....A # ....A. # ....A. # ....A. # ....A. # ....A. # ...


    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ..m.mm # ..m.mm # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ..m.m. # ..m... # ..m.mm # ..m..m # ..m..m # ...... # ...... # ...... # ...... # ...... #
    # ..m... # ..m... # ..m... # ..m... # ..m... # ..m..m # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ..m... # ..m... # ..m... # ..m... # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ...... # ...... # ...... # ..m... # ..m... # ..m... # ...m.. # ...m.. #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... # ...... #
... # ....A. # ....A. # ....A. # ....A. # .....A # .....A # .....A # ....A. # ....A. # ...A.. # ...


    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...m.. # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...m.. # ...m.. # ...m.. # ...... #
    # ...... # ...m.. # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
    # ...... # ...... # ...... # ...... #
... # ...A.. # ...A.. # ...A.. # ...A.. #
\end{Verbatim}

\subsection{Chase}
\subsubsection{Description}
In this domain an agent has to chase an adversary. The goal is defined by the agent being in a adversary's adjacent cell. The set of possile actions for the adversary is ~\{$up$, $right$, $down$, $left\}$. The set of possile actions for the agent is ~\{$rightup$, $rightdown$, $leftup$, $leftdown\}$. The agent and the adversary can move only once per turn.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
#. . . . . .#
#. A . . . .#
#. . . . . .#
#. . . . T .#
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Dist[col-has-agent;adjacent_col;col-has-adversary] [k=4]
  (B) Dist[row-has-agent;adjacent_row;row-has-adversary] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Absolute vertical distance between the agent and the adversary\\
$B$. Absolute horizontal distance between the agent and the adversary
\subsubsection{Policy}

\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B>0 -> {A ↓, B ↓}
  2. A=0 AND B>0 -> {A ↑, B ↓}
  3. A>0 AND B=0 -> {A ↓, B ↑}
\end{Verbatim}

\subsubsection{Policy Description}
According to rule one, is good to decrease the horizontal and vertical distance from the angent to the adversary if possible, i.e. move in diagonal towards him. Following the rule two if the agent is in the column of the adversary it is good to decrease the vertical distance. Finally, following the rule three if the agent is in the row of the adversary it is good to decrease the horizonal distance.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
 # A..... # ...... # ...... # ...... # ...... # ...... # .....T # ....AT #
 # ...... # .A.... # .A.... # ...... # .....T # ...A.T # ...A.. # ...... #
 # .....T # .....T # ...... # ...... # ...... # ...... # ...... # ...... #
 # ...... # ...... # .....T # ..A..T # ..A... # ...... # ...... # ...... #
\end{Verbatim}

\subsection{Nim}
\subsubsection{Description}
This is the Nim version used in~\cite{silver2020few}.In this domain there are two stacks (columns) of tokens. The number of actions available in each state depends on the number of tokens, a valid action involves removing one of the tokens. When a token is removed, that token and all the tokens above it are removed as well. When a token is removed, the turn is changed and the other player must remove a token as an action if there is one. The objective of the game is to leave the opponent with no actions available on his turn, i.e. without any token. The agent and the adversary has one action per turn.
It is interesting to note that in this game not all states are solvable if the opponent plays optimally. Knowing that the expert policy is to leave the opponent with both stacks with the same number of tokens, this state with both stacks leveled will not be solvable. Therefore, in order to solve it, we have made use of the $Bad(state)$ feature, which allows us not to have to define a good transition for each state, and as an alternative to label it as a bad state. The advantage is that if the initial state is solvable, bad states can always be avoided. However the policy is not complete for all states, in particular there will be no good transition for states with the two token stacks leveled, as we will see below, which we can say is the optimal representation of the game strategy. The winning tactic in this game is to level the two stacks of tokens by removing the token next to an empty cell and diagonally above another token, como se puede ver en~\cite{silver2020few}. The intuition behind this tactic is that if we follow it we can always reach a final situation where the opponent is forced to remove all the tokens from one pile, then we can remove the remaining tokens from the other pile.


\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  . #
  # t  . #
  # t  . #
  # t  t #
  # t  t #
  # t  t #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Num[And(cell-has-token,Exists(left_cell,cell-has-token))] [k=5]
  (B) Num[And(cell-has-token,Forall(leftdown_cell,cell-has-token))] [k=5]
  (C) Num[And(cell-has-token,Forall(rightdown_cell,cell-has-token))] [k=5]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. Number of tokens with a token immediately to the left of it\\
$B$. Number of tokens s.t. any cell to the leftdown also has a token.\\
$C$. Number of tokens s.t. any cell to the rightdown also has a token.

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. B>0 AND C>0 -> {A ·, B ↓, C ↓}
\end{Verbatim}

\subsubsection{Policy Description}
The policy guarantees to leave both stacks with the same number of tokens. This is done by decreasing the number of tokens s.t. any cell to the leftdown and to the rightdown has a token, without changing the number of tokens with a token at left to them. The only way to fulfill this policy is to remove the token that has a token to its leftdown and an empty cell to its left, or to remove the token that has a token to its rightdown and an empty cell to its right.

As the policy clearly shows, there is no good transition, that includes a change in the number of tokens that a token has on its left. In other words, states where the same number of tokens in the two stacks cannot be preserved with some legal move are bad, and therefore they have no good transition. In fact according to the policy it is good to leave the opponent in these states bad, because there will be no legal move for the opponent to leave us with the same number of tokens in the two stacks.


\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #  .  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
 # t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  t  #  t  .  #  .  .  #
\end{Verbatim}

\subsection{Delivery}
\subsubsection{Description}

In this domain the agent has to pick up a missed pet and take it to a certain location, the destination. The pet has the option on its turn to escape from its captor to any of the cells adjacent to the pet, even if is holded by the agent it can escape. The goal is defined by the pet being droped at the destination cell, not holded by the agent. In this domain we use the additional predicate $holding_pet$ to indicate that the agent is holding the pet. The agent has two actions per turn and the adversary one, otherwise the game would not be solvable for all the states. The set of possile actions for the agent is ~\{$pick$, $drop$, $rightup$, $reightdown$, $leftup$, $leftdown\}$. The agent, in order to pick up the pet has to execute the action $pick$ adjacent to the the pet's cell. The agent, in order to drop the pet has to execute the action $drop$ while $holding_pet$, and the pet will be droped at de agent's cell. The set of possile actions for the pet consist in escape to one of the adjacent cells to it, i.e. at most eight possible actions depending on the position of the pet in the grid. The pet can not move into the delivery point.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  .  .  .  .  A #
  # .  D  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  .  .  .  . #
  # .  .  p  .  .  . #
  # .  .  .  .  .  . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Atom[holding_pet] [k=1]
  (B) Dist[cell-has-pet;adjacent_cell;cell-has-agent] [k=4]
  (C) Dist[cell-has-destiny;adjacent_cell;cell-has-agent] [k=4]
\end{Verbatim}

\subsubsection{Feature Descrition}
$A$. The agent is holding a pet
$B$. Manhattan distance between the agent and the pet\\
$C$. Manhattan distance between the agent and the destination

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A=0 AND B>0 -> {B ↓}
  2. A>0 AND C>0 -> {C ↓}
  3. A>0 AND C=0 -> {A ↓}
  4. A=0 AND B>0 -> {A ↑, B ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the first rule, it is good to move towards the pet if the agent is not holding it and the distance to it is greater than zero. According to the second rule, if the agent has the pet, and is at the destination, it is good to move towards the destination. According to the third rule of the policy, if the agent is holding the pet, and is at destination it is good to drop the pet. Finally, according to the fourth rule of the policy, if the agent does not have the pet, and is at the adjacent cell it is good pick it.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  p.... # p.... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... #
  ..... # .A... # .A... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... # ..... #
  A.... # ..... # ..... # ..A.. # .pA.. # ..A.. # ..... # ..... # ..... # ..... # ..... # ..... #
  ..... # ..... # ..... # ..... # ..... # ..... # ...A. # ..pA. # ...A. # ..... # ..... # ..... #
  ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ....D # ...pA # ....A # ....A #
\end{Verbatim}

\subsection{Shoot}
\subsubsection{Description}
In this domain an agent has to shoot an adversary. The goal is defined by the agent being alone on the grid, having finished with the adversary. The agent can execute the action $shoot$, but the opponent does not, so the opponent is able to win the game from any state of any instance if he knows the right strategy. The agent and the adversary share the set of actions ~\{$up$, $right$, $down$, $left\}$. To reach the goal state, the agent has to be in the same row or column as the adverary and execute the action $shoot$. When the agent executes the $shoot$ action on the adversary's line, the adversary disappears. We can define the goal state, as the adversary not being on the board. The agent have one actions per turn and the adversary two, so that the agent can stand in the opponent's row or column and shoot before the opponent escapes.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # . . . . #
  # A . . . #
  # . . T . #
  # . . . . #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[cell-has-adversary] [k=1]
  (B) Bool[And(row-has-adversary,row-has-agent)] [k=3]
  (C) Bool[And(row-has-adversary,Exists(adjacent_row,row-has-agent))] [k=5]
  (D) Dist[col-has-agent;adjacent_col;col-has-adversary] [k=4]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The adversary is in the board\\
$B$. The agent and the adversary are in the same row\\
$C$. The agent row is adjacent to the adversary row\\
$D$. Absolute horizontal distance between the agent and the adversary

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 AND D>0 -> {A ·, B ·, C ·, D ↓}
  2. A>0 AND B=0 AND C>0 AND D>0 -> {A ·, B ↑, C ↓, D ·}
  3. A>0 AND D=0 -> {A ↓}
  4. A>0 AND B>0 -> {A ↓}
\end{Verbatim}

\subsubsection{Policy Description}
According to the fourth rule of the policy, if the agent is in the same column as the agent it is good to decrease $A$, i.e. shoot the adversary. In the same way, according to the third rule, if the agent is in the same row as the adversary it is good to execute the action $shoot$. According to the second rule, if the agent is in the row adjacent to the adversary it is good to get in the same row. Finally, according to the first rule, if the agent is not in the row adjacent to the adversary, and is not in the same row or column, it is good to decrease the horizontal distance between the agent and the adversary. The first rule can also be read as, it is good for the agent to approach the adversary horizontally.

\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  # A....... # .A...... # ..A..... # ..A..... # ...A.... # ....A... # ....A... # .....A.. # ......A. #
  # ........ # ........ # ........ # ........ # ........ # ........ # .....T.. # ......T. # ......T. #
  # ........ # ........ # ........ # ........ # ........ # ........ # ........ # ........ # ........ #
  # .....T.. # .....T.. # .....T.. # ........ # ........ # ........ # ........ # ........ # ........ #
  # ........ # ........ # ........ # .....T.. # .....T.. # .....T.. # ........ # ........ # ........ # ...

     # ......A. # .....A.. # .....A.. #
     # ........ # ........ # ........ #
     # .....T.. # .....T.. # ........ #
     # ........ # ........ # ........ #
  ...# ........ # ........ # ........ #

\end{Verbatim}


\subsection{Checkmate in n with Queen}
\subsubsection{Description}


\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  K #
  # .  .  .  .  . #
  # .  .  .  .  . #
  # Q  .  .  .  . #
  # .  .  .  .  . #
\end{Verbatim}


\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (CM) Atom[checkmate] [k=1]
  (SM) Atom[stalemate] [k=1]
  (QbK) Atom[white_queen_between_kings] [k=1]
  (KE) Atom[black_king_closed_in_edge_by_white_queen] [k=1]
  (KD) Dist[cell-has-white_king;adjacent_cell;cell-has-black_king] [k=4]
  (BKhA) Bool[And(cell-attaked-by-black_king,Not(cell-attaked-by-white))] [k=4]
  (A) Num[And(cell-attaked-by-black_king,Not(cell-attaked-by-white_queen))] [k=4]
  (WQadjBK_row) Bool[And(row-has-white_queen,Exists(adjacent_row,row-has-black_king)))] [k=6]
  (WQadjBK_col) Bool[And(col-has-white_queen,Exists(adjacent_col,col-has-black_king)))] [k=6]
  (WQadjBK) Bool[And(cell-has-white_queen,Exists(adjacent_cell,cell-has-black_king)))] [k=6]
  (WQadjWK) Bool[And(cell-has-white_queen,Exists(adjacent_cell,cell-has-white_king)))] [k=6]
\end{Verbatim}


\subsubsection{All Transition Features}
This features reason about how the value of the features can change in all possible successors.
\begin{Verbatim}[fontsize=\footnotesize]
  (CA) Bool[can(A, DEC)] [k=2]
  (CCM) bool[can(CM, INC)] [k=2]
\end{Verbatim}


\subsubsection{Feature Description}
$CM$. Checkmate\\
$SM$. Stalemate\\
$KE$. Black king is closed in an edge of the table by the white queen\\
$QbK$. White queen is somewhere between kings, rows or columns\\
$A$. The number of cells attaked by black king and not attacked by white queen\\
$BKhA$. Exist a cell attaked by Black king and no attacked by white pieces\\
$Qaw$. White queen is attaked and without protection, i.e. $WQadjBK$ AND $WQadjWK$\\
$WQadjBK_line$. White queen is in an adjacent line to the black king, i.e. $WQadjBK_row$ OR $WQadjWK_col$\\
$CA$. It is possible to decrease the number of cells attaked by black king and not attacked by white queen\\
$CCM$. It is possible to do checkmate in the current state


\subsubsection{Policy}
  \begin{Verbatim}[fontsize=\footnotesize]
   1. KE=0, CA>0 -> {A ↓}
   2. KE=0, CA=0 -> {A ·, KD ·}
   3. BKhA=0, WQadjBK_line>0 -> {KD ·, WQadjBK_line ·}
   4. CCM=0, KE>0 -> {KD ↓}
   5. CCM=1 -> {CM ↑}
  \end{Verbatim}


\subsubsection{Policy Implicits}
The next features has to be in that certain value after the transition for classifing the transition as good, taking into account the policy rules.
  \begin{Verbatim}[fontsize=\footnotesize]
   Qaw=0, Qbk>0, SM=0
  \end{Verbatim}


\subsubsection{Policy description}
Rules 1 and 2 cover the cases in which the king is not enclosed in an edge of the file. The first rule states that it is good to take transitions that decrease the number of squares available to the black king by using the queen. The second rule says that if the first rule is not applicable it is good to take transitions that do not alter the number of squares available to the black king, without moving the white king. The third rule refers to those cases where the black king had moved to its only free square, and classifies as good those transitions in which the queen is moved so that it is not stalemate and remains on the line adjacent to the black king. Rule four covers cases in which the black king is locked on an edge by the queen, and it is good to move the white king closer. finally, rule 5 says that if it is possible to checkmate it is good to do so. It should be noted that thanks to the implicit parts of the policy the king will never be left tied up and unprotected, stalemate will never occur and the white king will always be placed and remain in a line that is between the lines of the kings, whether rows or columns.


\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  K.... # K.... # K.... # ..... # ..... # ..... # ..... # ..... # ..... #
  ..Q.. # ..... # ..... # ..K.. # ..K.. # ..K.. # ..... # ..... # ..... #
  ..... # ..... # ..... # ..... # ..... # ..... # ..K.. # ..K.. # ..K.. #
  ..... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # ...Q. #
  ....k # ....k # ...k. # ...k. # ..k.. # ...k. # ...k. # ....k # ....k #
\end{Verbatim}


\subsection{Checkmate in n with Rook}
\subsubsection{Description}


\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  K #
  # .  .  .  .  . #
  # .  .  .  .  . #
  # R  .  .  .  . #
  # .  .  .  .  . #
\end{Verbatim}


\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (CM) Atom[checkmate] [k=1]
  (SM) Atom[stalemate] [k=1]
  (QbK) Atom[white_rook_between_kings] [k=1]
  (KE) Atom[black_king_closed_in_edge_by_white_rook] [k=1]
  (KD) Dist[cell-has-white_king;adjacent_cell;cell-has-black_king] [k=4]
  (BKhA) Bool[And(cell-attaked-by-black_king,Not(cell-attaked-by-white))] [k=4]
  (A) Num[And(cell-attaked-by-black_king,Not(cell-attaked-by-white_rook))] [k=4]
  (WQadjBK_row) Bool[And(row-has-white_rook,Exists(adjacent_row,row-has-black_king)))] [k=6]
  (WQadjBK_col) Bool[And(col-has-white_rook,Exists(adjacent_col,col-has-black_king)))] [k=6]
  (WQadjBK) Bool[And(cell-has-white_rook,Exists(adjacent_cell,cell-has-black_king)))] [k=6]
  (WQadjWK) Bool[And(cell-has-white_rook,Exists(adjacent_cell,cell-has-white_king)))] [k=6]
\end{Verbatim}


\subsubsection{All Transition Features}
This features reason about how the value of the features can change in all possible successors.
\begin{Verbatim}[fontsize=\footnotesize]
  (CA) Bool[can(A, DEC)] [k=2]
  (CCM) bool[can(CM, INC)] [k=2]
\end{Verbatim}


\subsubsection{Feature Description}
$CM$. Checkmate\\
$SM$. Stalemate\\
$KE$. Black king is closed in an edge of the table by the white queen\\
$QbK$. White queen is somewhere between kings, rows or columns\\
$A$. The number of cells attaked by black king and not attacked by white queen\\
$BKhA$. Exist a cell attaked by Black king and no attacked by white pieces\\
$Qaw$. White queen is attaked and without protection, i.e. $WQadjBK$ AND $WQadjWK$\\
$WQadjBK_line$. White queen is in an adjacent line to the black king, i.e. $WQadjBK_row$ OR $WQadjWK_col$\\
$CA$. It is possible to decrease the number of cells attaked by black king and not attacked by white queen\\
$CCM$. It is possible to do checkmate in the current state


\subsubsection{Policy}
  \begin{Verbatim}[fontsize=\footnotesize]
   1. KE=0, CA>0 -> {A ↓}
   2. KE=0, CA=0 -> {A ·, KD ·}
   3. BKhA=0, WQadjBK_line>0 -> {KD ·, WQadjBK_line ·}
   4. CCM=0, KE>0 -> {KD ↓}
   5. CCM=1 -> {CM ↑}
  \end{Verbatim}


\subsubsection{Policy Implicits}
The next features has to be in that certain value after the transition for classifing the transition as good, taking into account the policy rules.
  \begin{Verbatim}[fontsize=\footnotesize]
   Qaw=0, Qbk>0, SM=0
  \end{Verbatim}


\subsubsection{Policy description}
Rules 1 and 2 cover the cases in which the king is not enclosed in an edge of the file. The first rule states that it is good to take transitions that decrease the number of squares available to the black king by using the queen. The second rule says that if the first rule is not applicable it is good to take transitions that do not alter the number of squares available to the black king, without moving the white king. The third rule refers to those cases where the black king had moved to its only free square, and classifies as good those transitions in which the queen is moved so that it is not stalemate and remains on the line adjacent to the black king. Rule four covers cases in which the black king is locked on an edge by the queen, and it is good to move the white king closer. finally, rule 5 says that if it is possible to checkmate it is good to do so. It should be noted that thanks to the implicit parts of the policy the king will never be left tied up and unprotected, stalemate will never occur and the white king will always be placed and remain in a line that is between the lines of the kings, whether rows or columns.


\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  K.... # K.... # K.... # ..... # ..... # ..... # ..... # ..... # ..... #
  ..Q.. # ..... # ..... # ..K.. # ..K.. # ..K.. # ..... # ..... # ..... #
  ..... # ..... # ..... # ..... # ..... # ..... # ..K.. # ..K.. # ..K.. #
  ..... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # Q.... # ...Q. #
  ....k # ....k # ...k. # ...k. # ..k.. # ...k. # ...k. # ....k # ....k #
\end{Verbatim}

\subsection{Checkmate in one with Queen}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the grid. In order to win the white pieces has to do a checkmate in one movement. The white pieces have the white-king and the white-queen, and black pieces have the black-king.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  Q #
\end{Verbatim}


\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_queen)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_king)] [k=3]
  (D) Bool[And(row-has-black_king,row-has-white_queen)] [k=3]
  (E) Bool[And(cell-has-black_king,Exists(adjacent_cell,cell-has-white_queen))] [k=5]
  (F) Bool[And(col-has-white_king,Exists(adjacent_col,col-has-white_queen))] [k=5]
  (G) Bool[And(row-has-black_king,Exists(adjacent_row,row-has-white_king))] [k=5]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The black-king and the white-king are in the same column\\
$B$. The black-king and the white-queen are in the same column\\
$C$. The black-king and the white-king are in the same row\\
$D$. The black-king and the white-king are in the same row\\
$E$. The black-king's cell and the white-queen's cell are adjacent\\
$F$. The white-king's column and the white-queen's column are adjacent\\
$G$. The black-king's column and the white-king's column are adjacent

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ·, F ·, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ↑, F ·, G ·}
  2. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ·, F ↓, G ·}
  3. A=0 AND B=0 AND C>0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·},
                                                            {A ·, B ↑, C ·, D ·, E ·, F ·, G ·}
  4. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, C ·, D ·, E ↑, F ↑, G ·}
  5. A>0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ·, E ↑, F ↓, G ·},
                                                            {A ·, B ·, C ·, D ↑, E ·, F ↓, G ·}
  6. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F>0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ↓, G ·}
  7. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G=0 -> {A ·, C ·, D ·, E ↑, F ·, G ·}
  8. A=0 AND B=0 AND C>0 AND D>0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↑, C ·, D ↓, E ·, F ·, G ·}
  9. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F=0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ↑, G ·}
  10. A=0 AND B=0 AND C=0 AND D=0 AND E=0 AND F>0 AND G>0 -> {A ·, B ·, C ·, D ↑, E ↑, F ·, G ·}
  11. A>0 AND B>0 AND C=0 AND D=0 AND E=0 AND F=0 AND G=0 -> {A ·, B ↓, C ·, D ↑, E ·, F ·, G ·}
\end{Verbatim}

\subsubsection{Policy description}
The policy is an abstraction of all the possible one-move checkmates that exist on a chessboard of any size with two kings and a queen. For example rule one tells us that if the white king is on the same column as the black king, it is good to put the white queen on the same row as the black king, avoiding squares where the white queen would be in a cell adjacent to the black king. Following on from the first rule, transitions where the white queen is placed on the same column as the black king, and in a cell adjacent to the black king, would also be good, i.e. it is good for the white queen to be placed between the white and black kings. This policy will generalise to any chessboard size board.

\subsubsection{Task instance distribution}
To generate all the possible states from which the queen can checkmate the black king in one, in a board of $NxM$, we follow the following procedure. We generate all the possible checkmates that can be given to the black king if it is in the first row, then we apply three rotations to each board, 90, 180 and 270 degrees. To generate the checkmate with the black king in the first row we must take into account that for each black king position in this row there are three possible positions of the white king that allow a checkmate in one. In all these possible positions of the two kings we place the king in all the squares where it can checkmate the black king, and eliminate those where it would be giving a check. For the training, one hundred random boards of five by fifty are selected. To check that the policy is complete, it is tested on all possible, for each board size $NxM$ where both $N$ and $M$ are numbers no greater than twenty randomly selected.
v
\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  K  .  .  #  .  .  .  .  .  .  .  K  .  .  #
# .  .  .  .  .  .  .  .  .  k  #  .  .  .  .  .  .  .  .  Q  k  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  Q  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
# .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\subsection{Checkmate in one with Rook}
\subsubsection{Description}
In this game the action space is equal to the number of cells in the board. In order to win the white pieces has to do a checkmate in one movement. In white pieces have the white-king and the white-rook, and black pieces have the black-king.

\subsubsection{Training instances}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  k  .  .  . #
  # .  .  .  .  . #
  # .  K  .  .  . #
  # .  .  .  .  . #
  # .  .  .  .  R #
\end{Verbatim}

\subsubsection{Features}
\begin{Verbatim}[fontsize=\footnotesize]
  (A) Bool[And(col-has-black_king,col-has-white_king)] [k=3]
  (B) Bool[And(col-has-black_king,col-has-white_rook)] [k=3]
  (C) Bool[And(row-has-black_king,row-has-white_rook)] [k=3]
\end{Verbatim}

\subsubsection{Feature Description}
$A$. The black-king and the white-king are in the same column\\
$B$. The black-king and the white-rook are in the same column\\
$C$. The black-king and the white-rook are in the same row

\subsubsection{Policy}
\begin{Verbatim}[fontsize=\footnotesize]
  1. A>0 AND B=0 AND C=0 -> {A ·, B ·, C ↑}
  2. A=0 AND B=0 AND C=0 -> {A ·, B ↑, C ·}
\end{Verbatim}

\subsubsection{Policy description}
The policy is an abstraction of all the possible one-move checkmates that exist on a chessboard of any size with two kings and a tower. According to rule one, if the black king is in the same column as the white king, it is good to put the white rook in the same row as the white king. According to the second rule if the black king is not on the same column as the white king, it is good to put the white rook on the same column as the white king. This policy will generalise to any chessboard size board where checkmate is possible in one with rook and king.

\subsubsection{Task instance distribution}
To generate all the possible states from which the rook can checkmate the black king in one, in a board of $NxM$, we follow the following procedure. We generate all the possible checkmates that can be given to the black king if it is in the first row, then we apply three rotations to each board, 90, 180 and 270 degrees. To generate the checkmate with the black king in the first row we must take into account that for each black king position in this row there are one possible position of the white king that allow a checkmate in one. In all these possible positions of the two kings we place the king in all the squares where it can checkmate the black king, and eliminate those where it would be giving a check. For the training, one hundred random boards of five by fifty are selected. To check that the policy is complete, it is tested on all possible, for each board size $NxM$ where both $N$ and $M$ are numbers no greater than twenty randomly selected.


\subsubsection{Example of Game}
\begin{Verbatim}[fontsize=\footnotesize]
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  K  .  k  #  .  .  .  .  .  .  .  K  .  k  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
  # .  .  R  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  R  #
  # .  .  .  .  .  .  .  .  .  .  #  .  .  .  .  .  .  .  .  .  .  #
\end{Verbatim}

\newpage

\paragraph{Bullet points}
\begin{itemize}
\item Learning generalized policies in unsupervised FOND domains from small examples.
\item FOND encoding, introducing Good(s, a) features to deal with non-determinism. Where $a \in A$, and $A$ is not required to be fixed. However, if $A$ is fixed we show a way to exploit it reducing the number of transitions to distinguish.
\item Being complete in domains where not all states have a good transition, with the optional constraint Bad(s).
\item Empirical results with six different domains: Chase, Shoot, Space Invaders, Delivery, Checkmate with Queen, Checkmate with Rook.
\item Incremental policy learning, to address the scalability problem. First a random sample is taken from the whole graph and the policy is then refined incrementally by iteratively increasing the sample size. The strategy for increasing the size of the sample can be goal-directed or random.
%\item Introducing the sort of novelty to choose between the different successors classified as good by our policy
\item Use of a uniform grammar for the different grid domains.
\end{itemize}


\bibliographystyle{plain}
% Cross-referenced entries need to go after the entries that cross-reference them
\bibliography{abbrv-short,literatur,references,crossref-short}
\end{document}
